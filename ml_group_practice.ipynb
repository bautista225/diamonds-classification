{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant DecisionTreeClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant KNeighborsClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant MLPClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant VotingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant StackingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant BaggingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant AdaBoostClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant GradientBoostingClassifier. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant RandomForestClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "# Loading assignments functions.\n",
    "\n",
    "include(\"assignments_functions.jl\");\n",
    "include(\"practice_functions.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "\n",
    "#import Pkg; \n",
    "#Pkg.add(\"CSV\");\n",
    "#Pkg.add(\"DataFrames\");\n",
    "\n",
    "using CSV;\n",
    "using DataFrames;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>53,940 rows × 11 columns (omitted printing of 2 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>depth</th><th>table</th><th>price</th><th>x</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"String15\">String15</th><th title=\"String1\">String1</th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>1</td><td>0.23</td><td>Ideal</td><td>E</td><td>SI2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>2</td><td>0.21</td><td>Premium</td><td>E</td><td>SI1</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>3</td><td>0.23</td><td>Good</td><td>E</td><td>VS1</td><td>56.9</td><td>65.0</td><td>327</td><td>4.05</td></tr><tr><th>4</th><td>4</td><td>0.29</td><td>Premium</td><td>I</td><td>VS2</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>5</th><td>5</td><td>0.31</td><td>Good</td><td>J</td><td>SI2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>6</th><td>6</td><td>0.24</td><td>Very Good</td><td>J</td><td>VVS2</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>7</th><td>7</td><td>0.24</td><td>Very Good</td><td>I</td><td>VVS1</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>8</th><td>8</td><td>0.26</td><td>Very Good</td><td>H</td><td>SI1</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>9</th><td>9</td><td>0.22</td><td>Fair</td><td>E</td><td>VS2</td><td>65.1</td><td>61.0</td><td>337</td><td>3.87</td></tr><tr><th>10</th><td>10</td><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>11</th><td>11</td><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>12</th><td>12</td><td>0.23</td><td>Ideal</td><td>J</td><td>VS1</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>13</th><td>13</td><td>0.22</td><td>Premium</td><td>F</td><td>SI1</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>14</th><td>14</td><td>0.31</td><td>Ideal</td><td>J</td><td>SI2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>15</th><td>15</td><td>0.2</td><td>Premium</td><td>E</td><td>SI2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>16</th><td>16</td><td>0.32</td><td>Premium</td><td>E</td><td>I1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>17</th><td>17</td><td>0.3</td><td>Ideal</td><td>I</td><td>SI2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>18</th><td>18</td><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>19</th><td>19</td><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>20</th><td>20</td><td>0.3</td><td>Very Good</td><td>J</td><td>SI1</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>21</th><td>21</td><td>0.3</td><td>Good</td><td>I</td><td>SI2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>22</th><td>22</td><td>0.23</td><td>Very Good</td><td>E</td><td>VS2</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>23</th><td>23</td><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>24</th><td>24</td><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>25</th><td>25</td><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>58.1</td><td>62.0</td><td>353</td><td>4.44</td></tr><tr><th>26</th><td>26</td><td>0.23</td><td>Very Good</td><td>G</td><td>VVS2</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>27</th><td>27</td><td>0.24</td><td>Premium</td><td>I</td><td>VS1</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>28</th><td>28</td><td>0.3</td><td>Very Good</td><td>J</td><td>VS2</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>29</th><td>29</td><td>0.23</td><td>Very Good</td><td>D</td><td>VS2</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>30</th><td>30</td><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Column1 & carat & cut & color & clarity & depth & table & price & x & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & String15 & String1 & String7 & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & 0.23 & Ideal & E & SI2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 2 & 0.21 & Premium & E & SI1 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 3 & 0.23 & Good & E & VS1 & 56.9 & 65.0 & 327 & 4.05 & $\\dots$ \\\\\n",
       "\t4 & 4 & 0.29 & Premium & I & VS2 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t5 & 5 & 0.31 & Good & J & SI2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t6 & 6 & 0.24 & Very Good & J & VVS2 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t7 & 7 & 0.24 & Very Good & I & VVS1 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t8 & 8 & 0.26 & Very Good & H & SI1 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t9 & 9 & 0.22 & Fair & E & VS2 & 65.1 & 61.0 & 337 & 3.87 & $\\dots$ \\\\\n",
       "\t10 & 10 & 0.23 & Very Good & H & VS1 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t11 & 11 & 0.3 & Good & J & SI1 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t12 & 12 & 0.23 & Ideal & J & VS1 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t13 & 13 & 0.22 & Premium & F & SI1 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t14 & 14 & 0.31 & Ideal & J & SI2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t15 & 15 & 0.2 & Premium & E & SI2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t16 & 16 & 0.32 & Premium & E & I1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t17 & 17 & 0.3 & Ideal & I & SI2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t18 & 18 & 0.3 & Good & J & SI1 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t19 & 19 & 0.3 & Good & J & SI1 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t20 & 20 & 0.3 & Very Good & J & SI1 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t21 & 21 & 0.3 & Good & I & SI2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t22 & 22 & 0.23 & Very Good & E & VS2 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t23 & 23 & 0.23 & Very Good & H & VS1 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t24 & 24 & 0.31 & Very Good & J & SI1 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t25 & 25 & 0.31 & Very Good & J & SI1 & 58.1 & 62.0 & 353 & 4.44 & $\\dots$ \\\\\n",
       "\t26 & 26 & 0.23 & Very Good & G & VVS2 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t27 & 27 & 0.24 & Premium & I & VS1 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t28 & 28 & 0.3 & Very Good & J & VS2 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t29 & 29 & 0.23 & Very Good & D & VS2 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t30 & 30 & 0.23 & Very Good & F & VS1 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m53940×11 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m carat   \u001b[0m\u001b[1m cut       \u001b[0m\u001b[1m color   \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m depth   \u001b[0m\u001b[1m table   \u001b[0m\u001b[1m pric\u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Int64   \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m String15  \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int6\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │       1     0.23  Ideal      E        SI2         61.5     55.0    32 ⋯\n",
       "     2 │       2     0.21  Premium    E        SI1         59.8     61.0    32\n",
       "     3 │       3     0.23  Good       E        VS1         56.9     65.0    32\n",
       "     4 │       4     0.29  Premium    I        VS2         62.4     58.0    33\n",
       "     5 │       5     0.31  Good       J        SI2         63.3     58.0    33 ⋯\n",
       "     6 │       6     0.24  Very Good  J        VVS2        62.8     57.0    33\n",
       "     7 │       7     0.24  Very Good  I        VVS1        62.3     57.0    33\n",
       "     8 │       8     0.26  Very Good  H        SI1         61.9     55.0    33\n",
       "     9 │       9     0.22  Fair       E        VS2         65.1     61.0    33 ⋯\n",
       "    10 │      10     0.23  Very Good  H        VS1         59.4     61.0    33\n",
       "    11 │      11     0.3   Good       J        SI1         64.0     55.0    33\n",
       "   ⋮   │    ⋮        ⋮         ⋮         ⋮        ⋮        ⋮        ⋮       ⋮  ⋱\n",
       " 53931 │   53931     0.71  Premium    E        SI1         60.5     55.0   275\n",
       " 53932 │   53932     0.71  Premium    F        SI1         59.8     62.0   275 ⋯\n",
       " 53933 │   53933     0.7   Very Good  E        VS2         60.5     59.0   275\n",
       " 53934 │   53934     0.7   Very Good  E        VS2         61.2     59.0   275\n",
       " 53935 │   53935     0.72  Premium    D        SI1         62.7     59.0   275\n",
       " 53936 │   53936     0.72  Ideal      D        SI1         60.8     57.0   275 ⋯\n",
       " 53937 │   53937     0.72  Good       D        SI1         63.1     55.0   275\n",
       " 53938 │   53938     0.7   Very Good  D        SI1         62.8     60.0   275\n",
       " 53939 │   53939     0.86  Premium    H        SI2         61.0     58.0   275\n",
       " 53940 │   53940     0.75  Ideal      D        SI2         62.2     55.0   275 ⋯\n",
       "\u001b[36m                                                4 columns and 53919 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading csv file as dataframe.\n",
    "\n",
    "df = DataFrame(CSV.File(\"diamonds.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>11 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>Column1</td><td>26970.5</td><td>1</td><td>26970.5</td><td>53940</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>carat</td><td>0.79794</td><td>0.2</td><td>0.7</td><td>5.01</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>cut</td><td></td><td>Fair</td><td></td><td>Very Good</td><td>0</td><td>String15</td></tr><tr><th>4</th><td>color</td><td></td><td>D</td><td></td><td>J</td><td>0</td><td>String1</td></tr><tr><th>5</th><td>clarity</td><td></td><td>I1</td><td></td><td>VVS2</td><td>0</td><td>String7</td></tr><tr><th>6</th><td>depth</td><td>61.7494</td><td>43.0</td><td>61.8</td><td>79.0</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>table</td><td>57.4572</td><td>43.0</td><td>57.0</td><td>95.0</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>price</td><td>3932.8</td><td>326</td><td>2401.0</td><td>18823</td><td>0</td><td>Int64</td></tr><tr><th>9</th><td>x</td><td>5.73116</td><td>0.0</td><td>5.7</td><td>10.74</td><td>0</td><td>Float64</td></tr><tr><th>10</th><td>y</td><td>5.73453</td><td>0.0</td><td>5.71</td><td>58.9</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>z</td><td>3.53873</td><td>0.0</td><td>3.53</td><td>31.8</td><td>0</td><td>Float64</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Column1 & 26970.5 & 1 & 26970.5 & 53940 & 0 & Int64 \\\\\n",
       "\t2 & carat & 0.79794 & 0.2 & 0.7 & 5.01 & 0 & Float64 \\\\\n",
       "\t3 & cut &  & Fair &  & Very Good & 0 & String15 \\\\\n",
       "\t4 & color &  & D &  & J & 0 & String1 \\\\\n",
       "\t5 & clarity &  & I1 &  & VVS2 & 0 & String7 \\\\\n",
       "\t6 & depth & 61.7494 & 43.0 & 61.8 & 79.0 & 0 & Float64 \\\\\n",
       "\t7 & table & 57.4572 & 43.0 & 57.0 & 95.0 & 0 & Float64 \\\\\n",
       "\t8 & price & 3932.8 & 326 & 2401.0 & 18823 & 0 & Int64 \\\\\n",
       "\t9 & x & 5.73116 & 0.0 & 5.7 & 10.74 & 0 & Float64 \\\\\n",
       "\t10 & y & 5.73453 & 0.0 & 5.71 & 58.9 & 0 & Float64 \\\\\n",
       "\t11 & z & 3.53873 & 0.0 & 3.53 & 31.8 & 0 & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m11×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean    \u001b[0m\u001b[1m min  \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max       \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any  \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any       \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼─────────────────────────────────────────────────────────────────\n",
       "   1 │ Column1   26970.5  1     26970.5  53940             0  Int64\n",
       "   2 │ carat     0.79794  0.2   0.7      5.01              0  Float64\n",
       "   3 │ cut      \u001b[90m         \u001b[0m Fair \u001b[90m         \u001b[0m Very Good         0  String15\n",
       "   4 │ color    \u001b[90m         \u001b[0m D    \u001b[90m         \u001b[0m J                 0  String1\n",
       "   5 │ clarity  \u001b[90m         \u001b[0m I1   \u001b[90m         \u001b[0m VVS2              0  String7\n",
       "   6 │ depth     61.7494  43.0  61.8     79.0              0  Float64\n",
       "   7 │ table     57.4572  43.0  57.0     95.0              0  Float64\n",
       "   8 │ price     3932.8   326   2401.0   18823             0  Int64\n",
       "   9 │ x         5.73116  0.0   5.7      10.74             0  Float64\n",
       "  10 │ y         5.73453  0.0   5.71     58.9              0  Float64\n",
       "  11 │ z         3.53873  0.0   3.53     31.8              0  Float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>53,940 rows × 10 columns (omitted printing of 2 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>total_depth_percentage</th><th>table</th><th>price</th><th>length</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"String15\">String15</th><th title=\"String1\">String1</th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.23</td><td>Ideal</td><td>E</td><td>SI2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>0.21</td><td>Premium</td><td>E</td><td>SI1</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>0.23</td><td>Good</td><td>E</td><td>VS1</td><td>56.9</td><td>65.0</td><td>327</td><td>4.05</td></tr><tr><th>4</th><td>0.29</td><td>Premium</td><td>I</td><td>VS2</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>5</th><td>0.31</td><td>Good</td><td>J</td><td>SI2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>6</th><td>0.24</td><td>Very Good</td><td>J</td><td>VVS2</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>7</th><td>0.24</td><td>Very Good</td><td>I</td><td>VVS1</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>8</th><td>0.26</td><td>Very Good</td><td>H</td><td>SI1</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>9</th><td>0.22</td><td>Fair</td><td>E</td><td>VS2</td><td>65.1</td><td>61.0</td><td>337</td><td>3.87</td></tr><tr><th>10</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>11</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>12</th><td>0.23</td><td>Ideal</td><td>J</td><td>VS1</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>13</th><td>0.22</td><td>Premium</td><td>F</td><td>SI1</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>14</th><td>0.31</td><td>Ideal</td><td>J</td><td>SI2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>15</th><td>0.2</td><td>Premium</td><td>E</td><td>SI2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>16</th><td>0.32</td><td>Premium</td><td>E</td><td>I1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>17</th><td>0.3</td><td>Ideal</td><td>I</td><td>SI2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>18</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>19</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>20</th><td>0.3</td><td>Very Good</td><td>J</td><td>SI1</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>21</th><td>0.3</td><td>Good</td><td>I</td><td>SI2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>22</th><td>0.23</td><td>Very Good</td><td>E</td><td>VS2</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>23</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>24</th><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>25</th><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>58.1</td><td>62.0</td><td>353</td><td>4.44</td></tr><tr><th>26</th><td>0.23</td><td>Very Good</td><td>G</td><td>VVS2</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>27</th><td>0.24</td><td>Premium</td><td>I</td><td>VS1</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>28</th><td>0.3</td><td>Very Good</td><td>J</td><td>VS2</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>29</th><td>0.23</td><td>Very Good</td><td>D</td><td>VS2</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>30</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& carat & cut & color & clarity & total\\_depth\\_percentage & table & price & length & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & String15 & String1 & String7 & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.23 & Ideal & E & SI2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 0.21 & Premium & E & SI1 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 0.23 & Good & E & VS1 & 56.9 & 65.0 & 327 & 4.05 & $\\dots$ \\\\\n",
       "\t4 & 0.29 & Premium & I & VS2 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t5 & 0.31 & Good & J & SI2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t6 & 0.24 & Very Good & J & VVS2 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t7 & 0.24 & Very Good & I & VVS1 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t8 & 0.26 & Very Good & H & SI1 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t9 & 0.22 & Fair & E & VS2 & 65.1 & 61.0 & 337 & 3.87 & $\\dots$ \\\\\n",
       "\t10 & 0.23 & Very Good & H & VS1 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t11 & 0.3 & Good & J & SI1 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t12 & 0.23 & Ideal & J & VS1 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t13 & 0.22 & Premium & F & SI1 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t14 & 0.31 & Ideal & J & SI2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t15 & 0.2 & Premium & E & SI2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t16 & 0.32 & Premium & E & I1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t17 & 0.3 & Ideal & I & SI2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t18 & 0.3 & Good & J & SI1 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t19 & 0.3 & Good & J & SI1 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t20 & 0.3 & Very Good & J & SI1 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t21 & 0.3 & Good & I & SI2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t22 & 0.23 & Very Good & E & VS2 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t23 & 0.23 & Very Good & H & VS1 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t24 & 0.31 & Very Good & J & SI1 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t25 & 0.31 & Very Good & J & SI1 & 58.1 & 62.0 & 353 & 4.44 & $\\dots$ \\\\\n",
       "\t26 & 0.23 & Very Good & G & VVS2 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t27 & 0.24 & Premium & I & VS1 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t28 & 0.3 & Very Good & J & VS2 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t29 & 0.23 & Very Good & D & VS2 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t30 & 0.23 & Very Good & F & VS1 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m53940×10 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m carat   \u001b[0m\u001b[1m cut       \u001b[0m\u001b[1m color   \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m table  \u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m String15  \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │    0.23  Ideal      E        SI2                        61.5     55.0 ⋯\n",
       "     2 │    0.21  Premium    E        SI1                        59.8     61.0\n",
       "     3 │    0.23  Good       E        VS1                        56.9     65.0\n",
       "     4 │    0.29  Premium    I        VS2                        62.4     58.0\n",
       "     5 │    0.31  Good       J        SI2                        63.3     58.0 ⋯\n",
       "     6 │    0.24  Very Good  J        VVS2                       62.8     57.0\n",
       "     7 │    0.24  Very Good  I        VVS1                       62.3     57.0\n",
       "     8 │    0.26  Very Good  H        SI1                        61.9     55.0\n",
       "     9 │    0.22  Fair       E        VS2                        65.1     61.0 ⋯\n",
       "    10 │    0.23  Very Good  H        VS1                        59.4     61.0\n",
       "    11 │    0.3   Good       J        SI1                        64.0     55.0\n",
       "   ⋮   │    ⋮         ⋮         ⋮        ⋮               ⋮                ⋮    ⋱\n",
       " 53931 │    0.71  Premium    E        SI1                        60.5     55.0\n",
       " 53932 │    0.71  Premium    F        SI1                        59.8     62.0 ⋯\n",
       " 53933 │    0.7   Very Good  E        VS2                        60.5     59.0\n",
       " 53934 │    0.7   Very Good  E        VS2                        61.2     59.0\n",
       " 53935 │    0.72  Premium    D        SI1                        62.7     59.0\n",
       " 53936 │    0.72  Ideal      D        SI1                        60.8     57.0 ⋯\n",
       " 53937 │    0.72  Good       D        SI1                        63.1     55.0\n",
       " 53938 │    0.7   Very Good  D        SI1                        62.8     60.0\n",
       " 53939 │    0.86  Premium    H        SI2                        61.0     58.0\n",
       " 53940 │    0.75  Ideal      D        SI2                        62.2     55.0 ⋯\n",
       "\u001b[36m                                                4 columns and 53919 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As Column1 has the same indices as dataframe indices, we delete Column1.\n",
    "df_filtered = df[!,names(df)[names(df) .!= \"Column1\"]]\n",
    "\n",
    "# Rename of some columns in order to give more information about what contains each feature.\n",
    "rename!(df_filtered, Dict(:depth => :total_depth_percentage, :x => :length, :y => :width, :z => :depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: [:cut, :color, :clarity]\n",
      "Numerical features: [:carat, :total_depth_percentage, :table, :price, :length, :width, :depth]\n"
     ]
    }
   ],
   "source": [
    "# Getting pointers of categorical columns and numerical columns.\n",
    "\n",
    "categorical_columns, numerical_columns = getCategoricalNumericalFeatures(df_filtered)\n",
    "\n",
    "println(\"Categorical features: $(categorical_columns)\")\n",
    "println(\"Numerical features: $(numerical_columns)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceros for column total_depth_percentage: 0 over a total of 53940\n",
      "Ceros for column table: 0 over a total of 53940\n",
      "Ceros for column price: 0 over a total of 53940\n",
      "Ceros for column carat: 0 over a total of 53940\n",
      "Ceros for column depth: 20 over a total of 53940\n",
      "Ceros for column length: 8 over a total of 53940\n",
      "Ceros for column width: 7 over a total of 53940\n"
     ]
    }
   ],
   "source": [
    "# Checking if there are numerical features with ceros when they shouldn't. \n",
    "\n",
    "zerosPerColumn = getNumberOfZerosPerColumn(df_filtered, numerical_columns)\n",
    "\n",
    "for column in keys(zerosPerColumn)    \n",
    "    numberOfZeros, lengthOfColumn = zerosPerColumn[column]\n",
    "    println(\"Zeros for column $(column): $(numberOfZeros) over a total of $(lengthOfColumn)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>53,920 rows × 10 columns (omitted printing of 2 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>total_depth_percentage</th><th>table</th><th>price</th><th>length</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"String15\">String15</th><th title=\"String1\">String1</th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.23</td><td>Ideal</td><td>E</td><td>SI2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>0.21</td><td>Premium</td><td>E</td><td>SI1</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>0.23</td><td>Good</td><td>E</td><td>VS1</td><td>56.9</td><td>65.0</td><td>327</td><td>4.05</td></tr><tr><th>4</th><td>0.29</td><td>Premium</td><td>I</td><td>VS2</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>5</th><td>0.31</td><td>Good</td><td>J</td><td>SI2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>6</th><td>0.24</td><td>Very Good</td><td>J</td><td>VVS2</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>7</th><td>0.24</td><td>Very Good</td><td>I</td><td>VVS1</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>8</th><td>0.26</td><td>Very Good</td><td>H</td><td>SI1</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>9</th><td>0.22</td><td>Fair</td><td>E</td><td>VS2</td><td>65.1</td><td>61.0</td><td>337</td><td>3.87</td></tr><tr><th>10</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>11</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>12</th><td>0.23</td><td>Ideal</td><td>J</td><td>VS1</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>13</th><td>0.22</td><td>Premium</td><td>F</td><td>SI1</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>14</th><td>0.31</td><td>Ideal</td><td>J</td><td>SI2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>15</th><td>0.2</td><td>Premium</td><td>E</td><td>SI2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>16</th><td>0.32</td><td>Premium</td><td>E</td><td>I1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>17</th><td>0.3</td><td>Ideal</td><td>I</td><td>SI2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>18</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>19</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>20</th><td>0.3</td><td>Very Good</td><td>J</td><td>SI1</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>21</th><td>0.3</td><td>Good</td><td>I</td><td>SI2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>22</th><td>0.23</td><td>Very Good</td><td>E</td><td>VS2</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>23</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>24</th><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>25</th><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>58.1</td><td>62.0</td><td>353</td><td>4.44</td></tr><tr><th>26</th><td>0.23</td><td>Very Good</td><td>G</td><td>VVS2</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>27</th><td>0.24</td><td>Premium</td><td>I</td><td>VS1</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>28</th><td>0.3</td><td>Very Good</td><td>J</td><td>VS2</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>29</th><td>0.23</td><td>Very Good</td><td>D</td><td>VS2</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>30</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& carat & cut & color & clarity & total\\_depth\\_percentage & table & price & length & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & String15 & String1 & String7 & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.23 & Ideal & E & SI2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 0.21 & Premium & E & SI1 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 0.23 & Good & E & VS1 & 56.9 & 65.0 & 327 & 4.05 & $\\dots$ \\\\\n",
       "\t4 & 0.29 & Premium & I & VS2 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t5 & 0.31 & Good & J & SI2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t6 & 0.24 & Very Good & J & VVS2 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t7 & 0.24 & Very Good & I & VVS1 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t8 & 0.26 & Very Good & H & SI1 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t9 & 0.22 & Fair & E & VS2 & 65.1 & 61.0 & 337 & 3.87 & $\\dots$ \\\\\n",
       "\t10 & 0.23 & Very Good & H & VS1 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t11 & 0.3 & Good & J & SI1 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t12 & 0.23 & Ideal & J & VS1 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t13 & 0.22 & Premium & F & SI1 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t14 & 0.31 & Ideal & J & SI2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t15 & 0.2 & Premium & E & SI2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t16 & 0.32 & Premium & E & I1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t17 & 0.3 & Ideal & I & SI2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t18 & 0.3 & Good & J & SI1 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t19 & 0.3 & Good & J & SI1 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t20 & 0.3 & Very Good & J & SI1 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t21 & 0.3 & Good & I & SI2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t22 & 0.23 & Very Good & E & VS2 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t23 & 0.23 & Very Good & H & VS1 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t24 & 0.31 & Very Good & J & SI1 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t25 & 0.31 & Very Good & J & SI1 & 58.1 & 62.0 & 353 & 4.44 & $\\dots$ \\\\\n",
       "\t26 & 0.23 & Very Good & G & VVS2 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t27 & 0.24 & Premium & I & VS1 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t28 & 0.3 & Very Good & J & VS2 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t29 & 0.23 & Very Good & D & VS2 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t30 & 0.23 & Very Good & F & VS1 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m53920×10 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m carat   \u001b[0m\u001b[1m cut       \u001b[0m\u001b[1m color   \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m table  \u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m String15  \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │    0.23  Ideal      E        SI2                        61.5     55.0 ⋯\n",
       "     2 │    0.21  Premium    E        SI1                        59.8     61.0\n",
       "     3 │    0.23  Good       E        VS1                        56.9     65.0\n",
       "     4 │    0.29  Premium    I        VS2                        62.4     58.0\n",
       "     5 │    0.31  Good       J        SI2                        63.3     58.0 ⋯\n",
       "     6 │    0.24  Very Good  J        VVS2                       62.8     57.0\n",
       "     7 │    0.24  Very Good  I        VVS1                       62.3     57.0\n",
       "     8 │    0.26  Very Good  H        SI1                        61.9     55.0\n",
       "     9 │    0.22  Fair       E        VS2                        65.1     61.0 ⋯\n",
       "    10 │    0.23  Very Good  H        VS1                        59.4     61.0\n",
       "    11 │    0.3   Good       J        SI1                        64.0     55.0\n",
       "   ⋮   │    ⋮         ⋮         ⋮        ⋮               ⋮                ⋮    ⋱\n",
       " 53911 │    0.71  Premium    E        SI1                        60.5     55.0\n",
       " 53912 │    0.71  Premium    F        SI1                        59.8     62.0 ⋯\n",
       " 53913 │    0.7   Very Good  E        VS2                        60.5     59.0\n",
       " 53914 │    0.7   Very Good  E        VS2                        61.2     59.0\n",
       " 53915 │    0.72  Premium    D        SI1                        62.7     59.0\n",
       " 53916 │    0.72  Ideal      D        SI1                        60.8     57.0 ⋯\n",
       " 53917 │    0.72  Good       D        SI1                        63.1     55.0\n",
       " 53918 │    0.7   Very Good  D        SI1                        62.8     60.0\n",
       " 53919 │    0.86  Premium    H        SI2                        61.0     58.0\n",
       " 53920 │    0.75  Ideal      D        SI2                        62.2     55.0 ⋯\n",
       "\u001b[36m                                                4 columns and 53899 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting rows with zeros, as they are an small number over the size of the dataset .\n",
    "\n",
    "df_filtered_without_zeros = deleteRowsWithCeros(df_filtered, numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>10 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>carat</td><td>0.797698</td><td>0.2</td><td>0.7</td><td>5.01</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>cut</td><td></td><td>Fair</td><td></td><td>Very Good</td><td>0</td><td>String15</td></tr><tr><th>3</th><td>color</td><td></td><td>D</td><td></td><td>J</td><td>0</td><td>String1</td></tr><tr><th>4</th><td>clarity</td><td></td><td>I1</td><td></td><td>VVS2</td><td>0</td><td>String7</td></tr><tr><th>5</th><td>total_depth_percentage</td><td>61.7495</td><td>43.0</td><td>61.8</td><td>79.0</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>table</td><td>57.4568</td><td>43.0</td><td>57.0</td><td>95.0</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>price</td><td>3930.99</td><td>326</td><td>2401.0</td><td>18823</td><td>0</td><td>Int64</td></tr><tr><th>8</th><td>length</td><td>5.73163</td><td>3.73</td><td>5.7</td><td>10.74</td><td>0</td><td>Float64</td></tr><tr><th>9</th><td>width</td><td>5.73489</td><td>3.68</td><td>5.71</td><td>58.9</td><td>0</td><td>Float64</td></tr><tr><th>10</th><td>depth</td><td>3.54005</td><td>1.07</td><td>3.53</td><td>31.8</td><td>0</td><td>Float64</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & carat & 0.797698 & 0.2 & 0.7 & 5.01 & 0 & Float64 \\\\\n",
       "\t2 & cut &  & Fair &  & Very Good & 0 & String15 \\\\\n",
       "\t3 & color &  & D &  & J & 0 & String1 \\\\\n",
       "\t4 & clarity &  & I1 &  & VVS2 & 0 & String7 \\\\\n",
       "\t5 & total\\_depth\\_percentage & 61.7495 & 43.0 & 61.8 & 79.0 & 0 & Float64 \\\\\n",
       "\t6 & table & 57.4568 & 43.0 & 57.0 & 95.0 & 0 & Float64 \\\\\n",
       "\t7 & price & 3930.99 & 326 & 2401.0 & 18823 & 0 & Int64 \\\\\n",
       "\t8 & length & 5.73163 & 3.73 & 5.7 & 10.74 & 0 & Float64 \\\\\n",
       "\t9 & width & 5.73489 & 3.68 & 5.71 & 58.9 & 0 & Float64 \\\\\n",
       "\t10 & depth & 3.54005 & 1.07 & 3.53 & 31.8 & 0 & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m10×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable               \u001b[0m\u001b[1m mean     \u001b[0m\u001b[1m min  \u001b[0m\u001b[1m median \u001b[0m\u001b[1m max       \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m el\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol                 \u001b[0m\u001b[90m Union…   \u001b[0m\u001b[90m Any  \u001b[0m\u001b[90m Union… \u001b[0m\u001b[90m Any       \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m Da\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ carat                   0.797698  0.2   0.7     5.01              0  Fl ⋯\n",
       "   2 │ cut                    \u001b[90m          \u001b[0m Fair \u001b[90m        \u001b[0m Very Good         0  St\n",
       "   3 │ color                  \u001b[90m          \u001b[0m D    \u001b[90m        \u001b[0m J                 0  St\n",
       "   4 │ clarity                \u001b[90m          \u001b[0m I1   \u001b[90m        \u001b[0m VVS2              0  St\n",
       "   5 │ total_depth_percentage  61.7495   43.0  61.8    79.0              0  Fl ⋯\n",
       "   6 │ table                   57.4568   43.0  57.0    95.0              0  Fl\n",
       "   7 │ price                   3930.99   326   2401.0  18823             0  In\n",
       "   8 │ length                  5.73163   3.73  5.7     10.74             0  Fl\n",
       "   9 │ width                   5.73489   3.68  5.71    58.9              0  Fl ⋯\n",
       "  10 │ depth                   3.54005   1.07  3.53    31.8              0  Fl\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(df_filtered_without_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers for column carat: 1883 over 53920\n",
      "Extreme-outliers for column carat: 40 over 53920\n",
      "\n",
      "Outliers for column total_depth_percentage: 2543 over 53920\n",
      "Extreme-outliers for column total_depth_percentage: 278 over 53920\n",
      "\n",
      "Outliers for column table: 604 over 53920\n",
      "Extreme-outliers for column table: 28 over 53920\n",
      "\n",
      "Outliers for column price: 3532 over 53920\n",
      "Extreme-outliers for column price: 121 over 53920\n",
      "\n",
      "Outliers for column length: 24 over 53920\n",
      "Extreme-outliers for column length: 0 over 53920\n",
      "\n",
      "Outliers for column width: 22 over 53920\n",
      "Extreme-outliers for column width: 2 over 53920\n",
      "\n",
      "Outliers for column depth: 29 over 53920\n",
      "Extreme-outliers for column depth: 2 over 53920\n",
      "\n",
      "Rows to delete for extreme outliers 450\n",
      "Rows to delete for extreme outliers 6396\n",
      "Rows left 47524\n"
     ]
    }
   ],
   "source": [
    "for column in numerical_columns\n",
    "\n",
    "    column_data = df_filtered_without_zeros[!,column]\n",
    "\n",
    "    (outliers_indices, extreme_outliers_indices) = getBoundariesIndices(column_data)\n",
    "\n",
    "    println(\"Outliers for column $(column): $(length(column_data[outliers_indices])) over $(length(column_data))\")\n",
    "    println(\"Extreme-outliers for column $(column): $(length(column_data[extreme_outliers_indices])) over $(length(column_data))\\n\")\n",
    "end\n",
    "\n",
    "numberOfOutliers, numberOfExtremeOutliers = getNumberOfOutliers(df_filtered_without_zeros, numerical_columns)\n",
    "\n",
    "println(\"Rows to delete for extreme outliers $(numberOfExtremeOutliers)\")\n",
    "println(\"Rows to delete for extreme outliers $(numberOfOutliers)\")\n",
    "\n",
    "df_filtered_without_outliers = deleteOutliers(df_filtered_without_zeros, numerical_columns, deleteOnlyExtremeOutliers=false)\n",
    "\n",
    "println(\"Rows left $(size(df_filtered_without_outliers, 1))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated entries: 112\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>47,412 rows × 10 columns (omitted printing of 2 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>total_depth_percentage</th><th>table</th><th>price</th><th>length</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"String15\">String15</th><th title=\"String1\">String1</th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.23</td><td>Ideal</td><td>E</td><td>SI2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>0.21</td><td>Premium</td><td>E</td><td>SI1</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>0.29</td><td>Premium</td><td>I</td><td>VS2</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>4</th><td>0.31</td><td>Good</td><td>J</td><td>SI2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>5</th><td>0.24</td><td>Very Good</td><td>J</td><td>VVS2</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>6</th><td>0.24</td><td>Very Good</td><td>I</td><td>VVS1</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>7</th><td>0.26</td><td>Very Good</td><td>H</td><td>SI1</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>8</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>9</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>10</th><td>0.23</td><td>Ideal</td><td>J</td><td>VS1</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>11</th><td>0.22</td><td>Premium</td><td>F</td><td>SI1</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>12</th><td>0.31</td><td>Ideal</td><td>J</td><td>SI2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>13</th><td>0.2</td><td>Premium</td><td>E</td><td>SI2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>14</th><td>0.32</td><td>Premium</td><td>E</td><td>I1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>15</th><td>0.3</td><td>Ideal</td><td>I</td><td>SI2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>16</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>17</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>18</th><td>0.3</td><td>Very Good</td><td>J</td><td>SI1</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>19</th><td>0.3</td><td>Good</td><td>I</td><td>SI2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>20</th><td>0.23</td><td>Very Good</td><td>E</td><td>VS2</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>21</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>22</th><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>23</th><td>0.23</td><td>Very Good</td><td>G</td><td>VVS2</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>24</th><td>0.24</td><td>Premium</td><td>I</td><td>VS1</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>25</th><td>0.3</td><td>Very Good</td><td>J</td><td>VS2</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>26</th><td>0.23</td><td>Very Good</td><td>D</td><td>VS2</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>27</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>28</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.0</td><td>57.0</td><td>402</td><td>4.0</td></tr><tr><th>29</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>59.8</td><td>57.0</td><td>402</td><td>4.04</td></tr><tr><th>30</th><td>0.23</td><td>Very Good</td><td>E</td><td>VS1</td><td>60.7</td><td>59.0</td><td>402</td><td>3.97</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& carat & cut & color & clarity & total\\_depth\\_percentage & table & price & length & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & String15 & String1 & String7 & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.23 & Ideal & E & SI2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 0.21 & Premium & E & SI1 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 0.29 & Premium & I & VS2 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t4 & 0.31 & Good & J & SI2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t5 & 0.24 & Very Good & J & VVS2 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t6 & 0.24 & Very Good & I & VVS1 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t7 & 0.26 & Very Good & H & SI1 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t8 & 0.23 & Very Good & H & VS1 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t9 & 0.3 & Good & J & SI1 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t10 & 0.23 & Ideal & J & VS1 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t11 & 0.22 & Premium & F & SI1 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t12 & 0.31 & Ideal & J & SI2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t13 & 0.2 & Premium & E & SI2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t14 & 0.32 & Premium & E & I1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t15 & 0.3 & Ideal & I & SI2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t16 & 0.3 & Good & J & SI1 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t17 & 0.3 & Good & J & SI1 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t18 & 0.3 & Very Good & J & SI1 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t19 & 0.3 & Good & I & SI2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t20 & 0.23 & Very Good & E & VS2 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t21 & 0.23 & Very Good & H & VS1 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t22 & 0.31 & Very Good & J & SI1 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t23 & 0.23 & Very Good & G & VVS2 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t24 & 0.24 & Premium & I & VS1 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t25 & 0.3 & Very Good & J & VS2 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t26 & 0.23 & Very Good & D & VS2 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t27 & 0.23 & Very Good & F & VS1 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t28 & 0.23 & Very Good & F & VS1 & 60.0 & 57.0 & 402 & 4.0 & $\\dots$ \\\\\n",
       "\t29 & 0.23 & Very Good & F & VS1 & 59.8 & 57.0 & 402 & 4.04 & $\\dots$ \\\\\n",
       "\t30 & 0.23 & Very Good & E & VS1 & 60.7 & 59.0 & 402 & 3.97 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m47412×10 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m carat   \u001b[0m\u001b[1m cut       \u001b[0m\u001b[1m color   \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m table  \u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m String15  \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │    0.23  Ideal      E        SI2                        61.5     55.0 ⋯\n",
       "     2 │    0.21  Premium    E        SI1                        59.8     61.0\n",
       "     3 │    0.29  Premium    I        VS2                        62.4     58.0\n",
       "     4 │    0.31  Good       J        SI2                        63.3     58.0\n",
       "     5 │    0.24  Very Good  J        VVS2                       62.8     57.0 ⋯\n",
       "     6 │    0.24  Very Good  I        VVS1                       62.3     57.0\n",
       "     7 │    0.26  Very Good  H        SI1                        61.9     55.0\n",
       "     8 │    0.23  Very Good  H        VS1                        59.4     61.0\n",
       "     9 │    0.3   Good       J        SI1                        64.0     55.0 ⋯\n",
       "    10 │    0.23  Ideal      J        VS1                        62.8     56.0\n",
       "    11 │    0.22  Premium    F        SI1                        60.4     61.0\n",
       "   ⋮   │    ⋮         ⋮         ⋮        ⋮               ⋮                ⋮    ⋱\n",
       " 47403 │    0.71  Premium    E        SI1                        60.5     55.0\n",
       " 47404 │    0.71  Premium    F        SI1                        59.8     62.0 ⋯\n",
       " 47405 │    0.7   Very Good  E        VS2                        60.5     59.0\n",
       " 47406 │    0.7   Very Good  E        VS2                        61.2     59.0\n",
       " 47407 │    0.72  Premium    D        SI1                        62.7     59.0\n",
       " 47408 │    0.72  Ideal      D        SI1                        60.8     57.0 ⋯\n",
       " 47409 │    0.72  Good       D        SI1                        63.1     55.0\n",
       " 47410 │    0.7   Very Good  D        SI1                        62.8     60.0\n",
       " 47411 │    0.86  Premium    H        SI2                        61.0     58.0\n",
       " 47412 │    0.75  Ideal      D        SI2                        62.2     55.0 ⋯\n",
       "\u001b[36m                                                4 columns and 47391 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete Duplicates\n",
    "\n",
    "numberOfDuplicates = getNumberOfDuplicates(df_filtered_without_outliers)\n",
    "\n",
    "println(\"Number of duplicated entries: $(numberOfDuplicates)\")\n",
    "\n",
    "df_without_duplicates = deleteDuplicatedRows(df_filtered_without_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with a relative error greater or equal to 5.0% :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>22 rows × 6 columns (omitted printing of 1 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>cut</th><th>total_depth_percentage_check</th><th>total_depth_percentage</th><th>length</th><th>depth</th></tr><tr><th></th><th title=\"String15\">String15</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>Premium</td><td>68.1777</td><td>62.6</td><td>6.79</td><td>3.76</td></tr><tr><th>2</th><td>Very Good</td><td>51.245</td><td>62.8</td><td>6.26</td><td>3.19</td></tr><tr><th>3</th><td>Premium</td><td>66.5549</td><td>61.5</td><td>5.99</td><td>3.97</td></tr><tr><th>4</th><td>Ideal</td><td>65.8346</td><td>61.1</td><td>6.9</td><td>4.22</td></tr><tr><th>5</th><td>Ideal</td><td>65.8346</td><td>61.1</td><td>6.9</td><td>4.22</td></tr><tr><th>6</th><td>Premium</td><td>68.8372</td><td>61.6</td><td>6.64</td><td>3.7</td></tr><tr><th>7</th><td>Premium</td><td>55.0346</td><td>59.2</td><td>6.53</td><td>3.58</td></tr><tr><th>8</th><td>Ideal</td><td>20.4956</td><td>61.9</td><td>7.43</td><td>1.53</td></tr><tr><th>9</th><td>Ideal</td><td>19.4081</td><td>60.7</td><td>7.31</td><td>1.41</td></tr><tr><th>10</th><td>Premium</td><td>62.0408</td><td>59.0</td><td>7.39</td><td>4.56</td></tr><tr><th>11</th><td>Premium</td><td>65.7976</td><td>62.1</td><td>8.1</td><td>5.3</td></tr><tr><th>12</th><td>Premium</td><td>84.3305</td><td>61.2</td><td>4.51</td><td>4.44</td></tr><tr><th>13</th><td>Ideal</td><td>54.9189</td><td>61.4</td><td>4.64</td><td>2.54</td></tr><tr><th>14</th><td>Ideal</td><td>43.8765</td><td>60.9</td><td>4.71</td><td>2.06</td></tr><tr><th>15</th><td>Very Good</td><td>42.2535</td><td>61.0</td><td>5.31</td><td>2.25</td></tr><tr><th>16</th><td>Ideal</td><td>57.2183</td><td>62.7</td><td>5.16</td><td>3.25</td></tr><tr><th>17</th><td>Premium</td><td>56.0778</td><td>62.0</td><td>6.65</td><td>3.46</td></tr><tr><th>18</th><td>Very Good</td><td>100.697</td><td>63.7</td><td>5.01</td><td>5.06</td></tr><tr><th>19</th><td>Premium</td><td>59.1133</td><td>62.6</td><td>5.65</td><td>3.0</td></tr><tr><th>20</th><td>Premium</td><td>56.2197</td><td>61.2</td><td>6.65</td><td>3.48</td></tr><tr><th>21</th><td>Premium</td><td>65.0519</td><td>60.0</td><td>5.8</td><td>3.76</td></tr><tr><th>22</th><td>Premium</td><td>64.4584</td><td>60.0</td><td>5.62</td><td>3.6</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& cut & total\\_depth\\_percentage\\_check & total\\_depth\\_percentage & length & depth & \\\\\n",
       "\t\\hline\n",
       "\t& String15 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & Premium & 68.1777 & 62.6 & 6.79 & 3.76 & $\\dots$ \\\\\n",
       "\t2 & Very Good & 51.245 & 62.8 & 6.26 & 3.19 & $\\dots$ \\\\\n",
       "\t3 & Premium & 66.5549 & 61.5 & 5.99 & 3.97 & $\\dots$ \\\\\n",
       "\t4 & Ideal & 65.8346 & 61.1 & 6.9 & 4.22 & $\\dots$ \\\\\n",
       "\t5 & Ideal & 65.8346 & 61.1 & 6.9 & 4.22 & $\\dots$ \\\\\n",
       "\t6 & Premium & 68.8372 & 61.6 & 6.64 & 3.7 & $\\dots$ \\\\\n",
       "\t7 & Premium & 55.0346 & 59.2 & 6.53 & 3.58 & $\\dots$ \\\\\n",
       "\t8 & Ideal & 20.4956 & 61.9 & 7.43 & 1.53 & $\\dots$ \\\\\n",
       "\t9 & Ideal & 19.4081 & 60.7 & 7.31 & 1.41 & $\\dots$ \\\\\n",
       "\t10 & Premium & 62.0408 & 59.0 & 7.39 & 4.56 & $\\dots$ \\\\\n",
       "\t11 & Premium & 65.7976 & 62.1 & 8.1 & 5.3 & $\\dots$ \\\\\n",
       "\t12 & Premium & 84.3305 & 61.2 & 4.51 & 4.44 & $\\dots$ \\\\\n",
       "\t13 & Ideal & 54.9189 & 61.4 & 4.64 & 2.54 & $\\dots$ \\\\\n",
       "\t14 & Ideal & 43.8765 & 60.9 & 4.71 & 2.06 & $\\dots$ \\\\\n",
       "\t15 & Very Good & 42.2535 & 61.0 & 5.31 & 2.25 & $\\dots$ \\\\\n",
       "\t16 & Ideal & 57.2183 & 62.7 & 5.16 & 3.25 & $\\dots$ \\\\\n",
       "\t17 & Premium & 56.0778 & 62.0 & 6.65 & 3.46 & $\\dots$ \\\\\n",
       "\t18 & Very Good & 100.697 & 63.7 & 5.01 & 5.06 & $\\dots$ \\\\\n",
       "\t19 & Premium & 59.1133 & 62.6 & 5.65 & 3.0 & $\\dots$ \\\\\n",
       "\t20 & Premium & 56.2197 & 61.2 & 6.65 & 3.48 & $\\dots$ \\\\\n",
       "\t21 & Premium & 65.0519 & 60.0 & 5.8 & 3.76 & $\\dots$ \\\\\n",
       "\t22 & Premium & 64.4584 & 60.0 & 5.62 & 3.6 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m22×6 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m cut       \u001b[0m\u001b[1m total_depth_percentage_check \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m length\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String15  \u001b[0m\u001b[90m Float64                      \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float6\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ Premium                         68.1777                    62.6     6.7 ⋯\n",
       "   2 │ Very Good                       51.245                     62.8     6.2\n",
       "   3 │ Premium                         66.5549                    61.5     5.9\n",
       "   4 │ Ideal                           65.8346                    61.1     6.9\n",
       "   5 │ Ideal                           65.8346                    61.1     6.9 ⋯\n",
       "   6 │ Premium                         68.8372                    61.6     6.6\n",
       "   7 │ Premium                         55.0346                    59.2     6.5\n",
       "   8 │ Ideal                           20.4956                    61.9     7.4\n",
       "   9 │ Ideal                           19.4081                    60.7     7.3 ⋯\n",
       "  10 │ Premium                         62.0408                    59.0     7.3\n",
       "  11 │ Premium                         65.7976                    62.1     8.1\n",
       "  12 │ Premium                         84.3305                    61.2     4.5\n",
       "  13 │ Ideal                           54.9189                    61.4     4.6 ⋯\n",
       "  14 │ Ideal                           43.8765                    60.9     4.7\n",
       "  15 │ Very Good                       42.2535                    61.0     5.3\n",
       "  16 │ Ideal                           57.2183                    62.7     5.1\n",
       "  17 │ Premium                         56.0778                    62.0     6.6 ⋯\n",
       "  18 │ Very Good                      100.697                     63.7     5.0\n",
       "  19 │ Premium                         59.1133                    62.6     5.6\n",
       "  20 │ Premium                         56.2197                    61.2     6.6\n",
       "  21 │ Premium                         65.0519                    60.0     5.8 ⋯\n",
       "  22 │ Premium                         64.4584                    60.0     5.6\n",
       "\u001b[36m                                                               3 columns omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset without relative errors 47390 over 47412.\n"
     ]
    }
   ],
   "source": [
    "columns_to_previsualize = [:cut, :total_depth_percentage_check, :total_depth_percentage, :length, :depth, :width]\n",
    "\n",
    "error_rate = 0.05\n",
    "\n",
    "error_indices, df_with_check = getTotalDepthPercentageErrorIndices(df_without_duplicates, error_rate=error_rate)\n",
    "\n",
    "println(\"Rows with a relative error greater or equal to $(error_rate*100)% :\")\n",
    "display(df_with_check[error_indices, columns_to_previsualize])\n",
    "\n",
    "df_without_relative_errors = deleteTotalDepthPercentageErrors(df_without_duplicates, error_indices)\n",
    "\n",
    "println(\"Dataset without relative errors $(size(df_without_relative_errors,1)) over $(size(df_without_duplicates,1)).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Float64}:\n",
       " 38.20203\n",
       " 34.505856\n",
       " 46.72458\n",
       " 51.917249999999996\n",
       " 38.693952\n",
       " 38.830870000000004\n",
       " 42.32108100000001\n",
       " 38.718\n",
       " 49.6587\n",
       " 37.70442"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Volume variable\n",
    "\n",
    "df_with_volume = addVolumeColumntoDataframe(df_without_relative_errors)\n",
    "push!(numerical_columns, :volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dataframe(df_with_volume, categorical_columns, numerical_columns, save_charts=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>47,390 rows × 11 columns (omitted printing of 3 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>total_depth_percentage</th><th>table</th><th>price</th><th>length</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"String15\">String15</th><th title=\"String1\">String1</th><th title=\"String7\">String7</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.23</td><td>Ideal</td><td>E</td><td>SI2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>0.21</td><td>Premium</td><td>E</td><td>SI1</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>0.29</td><td>Premium</td><td>I</td><td>VS2</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>4</th><td>0.31</td><td>Good</td><td>J</td><td>SI2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>5</th><td>0.24</td><td>Very Good</td><td>J</td><td>VVS2</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>6</th><td>0.24</td><td>Very Good</td><td>I</td><td>VVS1</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>7</th><td>0.26</td><td>Very Good</td><td>H</td><td>SI1</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>8</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>9</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>10</th><td>0.23</td><td>Ideal</td><td>J</td><td>VS1</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>11</th><td>0.22</td><td>Premium</td><td>F</td><td>SI1</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>12</th><td>0.31</td><td>Ideal</td><td>J</td><td>SI2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>13</th><td>0.2</td><td>Premium</td><td>E</td><td>SI2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>14</th><td>0.32</td><td>Premium</td><td>E</td><td>I1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>15</th><td>0.3</td><td>Ideal</td><td>I</td><td>SI2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>16</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>17</th><td>0.3</td><td>Good</td><td>J</td><td>SI1</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>18</th><td>0.3</td><td>Very Good</td><td>J</td><td>SI1</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>19</th><td>0.3</td><td>Good</td><td>I</td><td>SI2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>20</th><td>0.23</td><td>Very Good</td><td>E</td><td>VS2</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>21</th><td>0.23</td><td>Very Good</td><td>H</td><td>VS1</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>22</th><td>0.31</td><td>Very Good</td><td>J</td><td>SI1</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>23</th><td>0.23</td><td>Very Good</td><td>G</td><td>VVS2</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>24</th><td>0.24</td><td>Premium</td><td>I</td><td>VS1</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>25</th><td>0.3</td><td>Very Good</td><td>J</td><td>VS2</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>26</th><td>0.23</td><td>Very Good</td><td>D</td><td>VS2</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>27</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>28</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>60.0</td><td>57.0</td><td>402</td><td>4.0</td></tr><tr><th>29</th><td>0.23</td><td>Very Good</td><td>F</td><td>VS1</td><td>59.8</td><td>57.0</td><td>402</td><td>4.04</td></tr><tr><th>30</th><td>0.23</td><td>Very Good</td><td>E</td><td>VS1</td><td>60.7</td><td>59.0</td><td>402</td><td>3.97</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& carat & cut & color & clarity & total\\_depth\\_percentage & table & price & length & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & String15 & String1 & String7 & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.23 & Ideal & E & SI2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 0.21 & Premium & E & SI1 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 0.29 & Premium & I & VS2 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t4 & 0.31 & Good & J & SI2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t5 & 0.24 & Very Good & J & VVS2 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t6 & 0.24 & Very Good & I & VVS1 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t7 & 0.26 & Very Good & H & SI1 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t8 & 0.23 & Very Good & H & VS1 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t9 & 0.3 & Good & J & SI1 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t10 & 0.23 & Ideal & J & VS1 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t11 & 0.22 & Premium & F & SI1 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t12 & 0.31 & Ideal & J & SI2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t13 & 0.2 & Premium & E & SI2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t14 & 0.32 & Premium & E & I1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t15 & 0.3 & Ideal & I & SI2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t16 & 0.3 & Good & J & SI1 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t17 & 0.3 & Good & J & SI1 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t18 & 0.3 & Very Good & J & SI1 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t19 & 0.3 & Good & I & SI2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t20 & 0.23 & Very Good & E & VS2 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t21 & 0.23 & Very Good & H & VS1 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t22 & 0.31 & Very Good & J & SI1 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t23 & 0.23 & Very Good & G & VVS2 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t24 & 0.24 & Premium & I & VS1 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t25 & 0.3 & Very Good & J & VS2 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t26 & 0.23 & Very Good & D & VS2 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t27 & 0.23 & Very Good & F & VS1 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t28 & 0.23 & Very Good & F & VS1 & 60.0 & 57.0 & 402 & 4.0 & $\\dots$ \\\\\n",
       "\t29 & 0.23 & Very Good & F & VS1 & 59.8 & 57.0 & 402 & 4.04 & $\\dots$ \\\\\n",
       "\t30 & 0.23 & Very Good & E & VS1 & 60.7 & 59.0 & 402 & 3.97 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m47390×11 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m carat   \u001b[0m\u001b[1m cut       \u001b[0m\u001b[1m color   \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m table  \u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m String15  \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m String7 \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float64\u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │    0.23  Ideal      E        SI2                        61.5     55.0 ⋯\n",
       "     2 │    0.21  Premium    E        SI1                        59.8     61.0\n",
       "     3 │    0.29  Premium    I        VS2                        62.4     58.0\n",
       "     4 │    0.31  Good       J        SI2                        63.3     58.0\n",
       "     5 │    0.24  Very Good  J        VVS2                       62.8     57.0 ⋯\n",
       "     6 │    0.24  Very Good  I        VVS1                       62.3     57.0\n",
       "     7 │    0.26  Very Good  H        SI1                        61.9     55.0\n",
       "     8 │    0.23  Very Good  H        VS1                        59.4     61.0\n",
       "     9 │    0.3   Good       J        SI1                        64.0     55.0 ⋯\n",
       "    10 │    0.23  Ideal      J        VS1                        62.8     56.0\n",
       "    11 │    0.22  Premium    F        SI1                        60.4     61.0\n",
       "   ⋮   │    ⋮         ⋮         ⋮        ⋮               ⋮                ⋮    ⋱\n",
       " 47381 │    0.71  Premium    E        SI1                        60.5     55.0\n",
       " 47382 │    0.71  Premium    F        SI1                        59.8     62.0 ⋯\n",
       " 47383 │    0.7   Very Good  E        VS2                        60.5     59.0\n",
       " 47384 │    0.7   Very Good  E        VS2                        61.2     59.0\n",
       " 47385 │    0.72  Premium    D        SI1                        62.7     59.0\n",
       " 47386 │    0.72  Ideal      D        SI1                        60.8     57.0 ⋯\n",
       " 47387 │    0.72  Good       D        SI1                        63.1     55.0\n",
       " 47388 │    0.7   Very Good  D        SI1                        62.8     60.0\n",
       " 47389 │    0.86  Premium    H        SI2                        61.0     58.0\n",
       " 47390 │    0.75  Ideal      D        SI2                        62.2     55.0 ⋯\n",
       "\u001b[36m                                                5 columns and 47369 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed = copy(df_with_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Ordinal encoding and one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>47,390 rows × 11 columns (omitted printing of 3 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>total_depth_percentage</th><th>table</th><th>price</th><th>length</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"String15\">String15</th><th title=\"Int64\">Int64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Int64\">Int64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.23</td><td>Ideal</td><td>6</td><td>2</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td></tr><tr><th>2</th><td>0.21</td><td>Premium</td><td>6</td><td>3</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td></tr><tr><th>3</th><td>0.29</td><td>Premium</td><td>2</td><td>4</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td></tr><tr><th>4</th><td>0.31</td><td>Good</td><td>1</td><td>2</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td></tr><tr><th>5</th><td>0.24</td><td>Very Good</td><td>1</td><td>6</td><td>62.8</td><td>57.0</td><td>336</td><td>3.94</td></tr><tr><th>6</th><td>0.24</td><td>Very Good</td><td>2</td><td>7</td><td>62.3</td><td>57.0</td><td>336</td><td>3.95</td></tr><tr><th>7</th><td>0.26</td><td>Very Good</td><td>3</td><td>3</td><td>61.9</td><td>55.0</td><td>337</td><td>4.07</td></tr><tr><th>8</th><td>0.23</td><td>Very Good</td><td>3</td><td>5</td><td>59.4</td><td>61.0</td><td>338</td><td>4.0</td></tr><tr><th>9</th><td>0.3</td><td>Good</td><td>1</td><td>3</td><td>64.0</td><td>55.0</td><td>339</td><td>4.25</td></tr><tr><th>10</th><td>0.23</td><td>Ideal</td><td>1</td><td>5</td><td>62.8</td><td>56.0</td><td>340</td><td>3.93</td></tr><tr><th>11</th><td>0.22</td><td>Premium</td><td>5</td><td>3</td><td>60.4</td><td>61.0</td><td>342</td><td>3.88</td></tr><tr><th>12</th><td>0.31</td><td>Ideal</td><td>1</td><td>2</td><td>62.2</td><td>54.0</td><td>344</td><td>4.35</td></tr><tr><th>13</th><td>0.2</td><td>Premium</td><td>6</td><td>2</td><td>60.2</td><td>62.0</td><td>345</td><td>3.79</td></tr><tr><th>14</th><td>0.32</td><td>Premium</td><td>6</td><td>1</td><td>60.9</td><td>58.0</td><td>345</td><td>4.38</td></tr><tr><th>15</th><td>0.3</td><td>Ideal</td><td>2</td><td>2</td><td>62.0</td><td>54.0</td><td>348</td><td>4.31</td></tr><tr><th>16</th><td>0.3</td><td>Good</td><td>1</td><td>3</td><td>63.4</td><td>54.0</td><td>351</td><td>4.23</td></tr><tr><th>17</th><td>0.3</td><td>Good</td><td>1</td><td>3</td><td>63.8</td><td>56.0</td><td>351</td><td>4.23</td></tr><tr><th>18</th><td>0.3</td><td>Very Good</td><td>1</td><td>3</td><td>62.7</td><td>59.0</td><td>351</td><td>4.21</td></tr><tr><th>19</th><td>0.3</td><td>Good</td><td>2</td><td>2</td><td>63.3</td><td>56.0</td><td>351</td><td>4.26</td></tr><tr><th>20</th><td>0.23</td><td>Very Good</td><td>6</td><td>4</td><td>63.8</td><td>55.0</td><td>352</td><td>3.85</td></tr><tr><th>21</th><td>0.23</td><td>Very Good</td><td>3</td><td>5</td><td>61.0</td><td>57.0</td><td>353</td><td>3.94</td></tr><tr><th>22</th><td>0.31</td><td>Very Good</td><td>1</td><td>3</td><td>59.4</td><td>62.0</td><td>353</td><td>4.39</td></tr><tr><th>23</th><td>0.23</td><td>Very Good</td><td>4</td><td>6</td><td>60.4</td><td>58.0</td><td>354</td><td>3.97</td></tr><tr><th>24</th><td>0.24</td><td>Premium</td><td>2</td><td>5</td><td>62.5</td><td>57.0</td><td>355</td><td>3.97</td></tr><tr><th>25</th><td>0.3</td><td>Very Good</td><td>1</td><td>4</td><td>62.2</td><td>57.0</td><td>357</td><td>4.28</td></tr><tr><th>26</th><td>0.23</td><td>Very Good</td><td>7</td><td>4</td><td>60.5</td><td>61.0</td><td>357</td><td>3.96</td></tr><tr><th>27</th><td>0.23</td><td>Very Good</td><td>5</td><td>5</td><td>60.9</td><td>57.0</td><td>357</td><td>3.96</td></tr><tr><th>28</th><td>0.23</td><td>Very Good</td><td>5</td><td>5</td><td>60.0</td><td>57.0</td><td>402</td><td>4.0</td></tr><tr><th>29</th><td>0.23</td><td>Very Good</td><td>5</td><td>5</td><td>59.8</td><td>57.0</td><td>402</td><td>4.04</td></tr><tr><th>30</th><td>0.23</td><td>Very Good</td><td>6</td><td>5</td><td>60.7</td><td>59.0</td><td>402</td><td>3.97</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& carat & cut & color & clarity & total\\_depth\\_percentage & table & price & length & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & String15 & Int64 & Int64 & Float64 & Float64 & Int64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.23 & Ideal & 6 & 2 & 61.5 & 55.0 & 326 & 3.95 & $\\dots$ \\\\\n",
       "\t2 & 0.21 & Premium & 6 & 3 & 59.8 & 61.0 & 326 & 3.89 & $\\dots$ \\\\\n",
       "\t3 & 0.29 & Premium & 2 & 4 & 62.4 & 58.0 & 334 & 4.2 & $\\dots$ \\\\\n",
       "\t4 & 0.31 & Good & 1 & 2 & 63.3 & 58.0 & 335 & 4.34 & $\\dots$ \\\\\n",
       "\t5 & 0.24 & Very Good & 1 & 6 & 62.8 & 57.0 & 336 & 3.94 & $\\dots$ \\\\\n",
       "\t6 & 0.24 & Very Good & 2 & 7 & 62.3 & 57.0 & 336 & 3.95 & $\\dots$ \\\\\n",
       "\t7 & 0.26 & Very Good & 3 & 3 & 61.9 & 55.0 & 337 & 4.07 & $\\dots$ \\\\\n",
       "\t8 & 0.23 & Very Good & 3 & 5 & 59.4 & 61.0 & 338 & 4.0 & $\\dots$ \\\\\n",
       "\t9 & 0.3 & Good & 1 & 3 & 64.0 & 55.0 & 339 & 4.25 & $\\dots$ \\\\\n",
       "\t10 & 0.23 & Ideal & 1 & 5 & 62.8 & 56.0 & 340 & 3.93 & $\\dots$ \\\\\n",
       "\t11 & 0.22 & Premium & 5 & 3 & 60.4 & 61.0 & 342 & 3.88 & $\\dots$ \\\\\n",
       "\t12 & 0.31 & Ideal & 1 & 2 & 62.2 & 54.0 & 344 & 4.35 & $\\dots$ \\\\\n",
       "\t13 & 0.2 & Premium & 6 & 2 & 60.2 & 62.0 & 345 & 3.79 & $\\dots$ \\\\\n",
       "\t14 & 0.32 & Premium & 6 & 1 & 60.9 & 58.0 & 345 & 4.38 & $\\dots$ \\\\\n",
       "\t15 & 0.3 & Ideal & 2 & 2 & 62.0 & 54.0 & 348 & 4.31 & $\\dots$ \\\\\n",
       "\t16 & 0.3 & Good & 1 & 3 & 63.4 & 54.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t17 & 0.3 & Good & 1 & 3 & 63.8 & 56.0 & 351 & 4.23 & $\\dots$ \\\\\n",
       "\t18 & 0.3 & Very Good & 1 & 3 & 62.7 & 59.0 & 351 & 4.21 & $\\dots$ \\\\\n",
       "\t19 & 0.3 & Good & 2 & 2 & 63.3 & 56.0 & 351 & 4.26 & $\\dots$ \\\\\n",
       "\t20 & 0.23 & Very Good & 6 & 4 & 63.8 & 55.0 & 352 & 3.85 & $\\dots$ \\\\\n",
       "\t21 & 0.23 & Very Good & 3 & 5 & 61.0 & 57.0 & 353 & 3.94 & $\\dots$ \\\\\n",
       "\t22 & 0.31 & Very Good & 1 & 3 & 59.4 & 62.0 & 353 & 4.39 & $\\dots$ \\\\\n",
       "\t23 & 0.23 & Very Good & 4 & 6 & 60.4 & 58.0 & 354 & 3.97 & $\\dots$ \\\\\n",
       "\t24 & 0.24 & Premium & 2 & 5 & 62.5 & 57.0 & 355 & 3.97 & $\\dots$ \\\\\n",
       "\t25 & 0.3 & Very Good & 1 & 4 & 62.2 & 57.0 & 357 & 4.28 & $\\dots$ \\\\\n",
       "\t26 & 0.23 & Very Good & 7 & 4 & 60.5 & 61.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t27 & 0.23 & Very Good & 5 & 5 & 60.9 & 57.0 & 357 & 3.96 & $\\dots$ \\\\\n",
       "\t28 & 0.23 & Very Good & 5 & 5 & 60.0 & 57.0 & 402 & 4.0 & $\\dots$ \\\\\n",
       "\t29 & 0.23 & Very Good & 5 & 5 & 59.8 & 57.0 & 402 & 4.04 & $\\dots$ \\\\\n",
       "\t30 & 0.23 & Very Good & 6 & 5 & 60.7 & 59.0 & 402 & 3.97 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m47390×11 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m carat   \u001b[0m\u001b[1m cut       \u001b[0m\u001b[1m color \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m table   \u001b[0m\u001b[1m \u001b[0m ⋯\n",
       "\u001b[1m       \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m String15  \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m \u001b[0m ⋯\n",
       "───────┼────────────────────────────────────────────────────────────────────────\n",
       "     1 │    0.23  Ideal          6        2                    61.5     55.0   ⋯\n",
       "     2 │    0.21  Premium        6        3                    59.8     61.0\n",
       "     3 │    0.29  Premium        2        4                    62.4     58.0\n",
       "     4 │    0.31  Good           1        2                    63.3     58.0\n",
       "     5 │    0.24  Very Good      1        6                    62.8     57.0   ⋯\n",
       "     6 │    0.24  Very Good      2        7                    62.3     57.0\n",
       "     7 │    0.26  Very Good      3        3                    61.9     55.0\n",
       "     8 │    0.23  Very Good      3        5                    59.4     61.0\n",
       "     9 │    0.3   Good           1        3                    64.0     55.0   ⋯\n",
       "    10 │    0.23  Ideal          1        5                    62.8     56.0\n",
       "    11 │    0.22  Premium        5        3                    60.4     61.0\n",
       "   ⋮   │    ⋮         ⋮        ⋮       ⋮               ⋮                ⋮      ⋱\n",
       " 47381 │    0.71  Premium        6        3                    60.5     55.0\n",
       " 47382 │    0.71  Premium        5        3                    59.8     62.0   ⋯\n",
       " 47383 │    0.7   Very Good      6        4                    60.5     59.0\n",
       " 47384 │    0.7   Very Good      6        4                    61.2     59.0\n",
       " 47385 │    0.72  Premium        7        3                    62.7     59.0\n",
       " 47386 │    0.72  Ideal          7        3                    60.8     57.0   ⋯\n",
       " 47387 │    0.72  Good           7        3                    63.1     55.0\n",
       " 47388 │    0.7   Very Good      7        3                    62.8     60.0\n",
       " 47389 │    0.86  Premium        3        2                    61.0     58.0\n",
       " 47390 │    0.75  Ideal          7        2                    62.2     55.0   ⋯\n",
       "\u001b[36m                                                5 columns and 47369 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordinal enconding\n",
    "\n",
    "df_ordinal_encoded = diamondsOrdinalEncoding(df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47390-element Vector{String15}:\n",
       " \"Ideal\"\n",
       " \"Premium\"\n",
       " \"Premium\"\n",
       " \"Good\"\n",
       " \"Very Good\"\n",
       " \"Very Good\"\n",
       " \"Very Good\"\n",
       " \"Very Good\"\n",
       " \"Good\"\n",
       " \"Ideal\"\n",
       " \"Premium\"\n",
       " \"Ideal\"\n",
       " \"Premium\"\n",
       " ⋮\n",
       " \"Premium\"\n",
       " \"Ideal\"\n",
       " \"Premium\"\n",
       " \"Premium\"\n",
       " \"Very Good\"\n",
       " \"Very Good\"\n",
       " \"Premium\"\n",
       " \"Ideal\"\n",
       " \"Good\"\n",
       " \"Very Good\"\n",
       " \"Premium\"\n",
       " \"Ideal\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OneHot encoding for target variable\n",
    "\n",
    "targets = df_preprocessed[!,:cut]\n",
    "targets = String15.(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\u001b[1m4739×10 DataFrame\u001b[0m\n",
       "\u001b[1m  Row \u001b[0m│\u001b[1m carat   \u001b[0m\u001b[1m color \u001b[0m\u001b[1m clarity \u001b[0m\u001b[1m total_depth_percentage \u001b[0m\u001b[1m table   \u001b[0m\u001b[1m price \u001b[0m\u001b[1m lengt\u001b[0m ⋯\n",
       "\u001b[1m      \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Float64                \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float\u001b[0m ⋯\n",
       "──────┼─────────────────────────────────────────────────────────────────────────\n",
       "    1 │    0.35      7        4                    62.5     55.0    829     4. ⋯\n",
       "    2 │    0.41      5        7                    61.8     54.0   1213     4.\n",
       "    3 │    0.9       4        6                    62.6     55.0   4668     6.\n",
       "    4 │    1.13      4        6                    61.4     58.0   8019     6.\n",
       "    5 │    0.91      6        3                    60.5     61.0   4211     6. ⋯\n",
       "    6 │    1.03      3        2                    62.3     59.0   4153     6.\n",
       "    7 │    0.33      6        4                    60.7     58.0    723     4.\n",
       "    8 │    0.46      7        5                    61.6     57.0   1637     4.\n",
       "    9 │    0.96      7        2                    60.2     63.0   3419     6. ⋯\n",
       "   10 │    0.23      7        5                    61.5     58.0    611     3.\n",
       "   11 │    0.31      3        4                    61.5     57.0    489     4.\n",
       "  ⋮   │    ⋮       ⋮       ⋮               ⋮                ⋮       ⋮       ⋮  ⋱\n",
       " 4730 │    1.01      4        2                    59.3     62.0   4397     6.\n",
       " 4731 │    0.3       4        7                    62.4     57.0   1013     4. ⋯\n",
       " 4732 │    0.91      2        5                    61.9     57.0   3669     6.\n",
       " 4733 │    1.01      2        2                    64.1     58.0   3411     6.\n",
       " 4734 │    0.9       3        3                    60.0     56.0   3629     6.\n",
       " 4735 │    0.54      5        3                    61.8     60.0   1330     5. ⋯\n",
       " 4736 │    1.5       1        4                    59.6     60.0   7368     7.\n",
       " 4737 │    0.36      2        2                    62.1     54.0    439     4.\n",
       " 4738 │    0.9       1        7                    62.0     58.0   3617     6.\n",
       " 4739 │    1.21      3        2                    62.9     58.0   4913     6. ⋯\n",
       "\u001b[36m                                                 4 columns and 4718 rows omitted\u001b[0m, String15[String15(\"Ideal\"); String15(\"Ideal\"); … ; String15(\"Premium\"); String15(\"Premium\");;])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = df_ordinal_encoded[!, Not(\"cut\")]\n",
    "output_data = targets\n",
    "\n",
    "Random.seed!(123)\n",
    "\n",
    "testRatio = 0.1\n",
    "N = size(input_data,1)\n",
    "\n",
    "(trainingIndex, testIndex) = holdOut(N, testRatio)\n",
    "\n",
    "train_input, train_output   = (input_data[trainingIndex,:], output_data[trainingIndex,:])\n",
    "test_input, test_output     = (input_data[testIndex,:],     output_data[testIndex,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Normalize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Union{Nothing, Int64}}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preserve initial indexes.\n",
    "\n",
    "column_names = names(train_input)\n",
    "\n",
    "numerical_indices = indexin([\"carat\", \"total_depth_percentage\", \"table\", \"price\", \"length\", \"width\", \"depth\", \"volume\"], column_names)\n",
    "ordinal_indices = indexin([\"color\",\"clarity\"], column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4739×8 Matrix{Float64}:\n",
       " -0.966023     0.645314   -1.11747   …  -1.02753   -0.987149  -0.959774\n",
       " -0.804266     0.0119125  -1.61234      -0.740022  -0.756016  -0.782446\n",
       "  0.51675      0.7358     -1.11747       0.656467   0.696821   0.507276\n",
       "  1.13682     -0.350031    0.367133      1.15961    1.14258    1.16435\n",
       "  0.543709    -1.1644      1.85173       0.69754    0.548235   0.493103\n",
       "  0.867223     0.464342    0.862     …   0.892638   0.960973   0.848715\n",
       " -1.01994     -0.983432    0.367133     -1.08914   -1.16875   -1.01573\n",
       " -0.669469    -0.169059   -0.127734     -0.585998  -0.607431  -0.667917\n",
       "  0.678507    -1.43586     2.84147       0.769418   0.630783   0.620934\n",
       " -1.28954     -0.259545    0.367133     -1.61283   -1.64753   -1.29248\n",
       " -1.07386     -0.259545   -0.127734  …  -1.2021    -1.23479   -1.07855\n",
       "  0.543709     0.464342    0.862         0.63593    0.663802   0.490752\n",
       " -0.992983    -2.25024     0.862        -1.02753   -1.18526   -0.982258\n",
       "  ⋮                                  ⋱   ⋮                    \n",
       "  2.35        -1.1644      0.367133      2.04269    1.88551    2.42647\n",
       "  0.00451885   0.373856   -1.06798       0.173857   0.201536   0.00485703\n",
       "  0.813304    -2.25024     2.3466        1.05693    0.74635    0.846969\n",
       " -1.10082      0.554828   -0.127734  …  -1.31505   -1.2513    -1.11055\n",
       "  0.543709     0.102398   -0.127734      0.656467   0.696821   0.555003\n",
       "  0.813304     2.09309     0.367133      0.728345   0.993992   0.729886\n",
       "  0.51675     -1.61683    -0.622601      0.738613   0.581254   0.57697\n",
       " -0.453793     0.0119125   1.35687      -0.360095  -0.343278  -0.468122\n",
       "  2.13432     -1.97878     1.35687   …   1.87839    1.57183    2.01668\n",
       " -0.939064     0.28337    -1.61234      -0.99673   -0.97064   -0.936933\n",
       "  0.51675      0.192884    0.367133      0.57432    0.630783   0.478051\n",
       "  1.35249      1.00726     0.367133      1.16988    1.32418    1.25636"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize numeric variables (zero mean)\n",
    "\n",
    "normalization_parameters = getZeroMeanNormalizationParameters(train_input, numerical_columns) # Only with train set.\n",
    "\n",
    "train_input_numerical_columns_zeroMean  = getZeroMeanNormalizedColumns(train_input, numerical_columns, normalization_parameters)\n",
    "test_input_numerical_columns_zeroMean   = getZeroMeanNormalizedColumns(test_input,  numerical_columns, normalization_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4739×2 Matrix{Float64}:\n",
       " 1.0       0.428571\n",
       " 0.666667  0.857143\n",
       " 0.5       0.714286\n",
       " 0.5       0.714286\n",
       " 0.833333  0.285714\n",
       " 0.333333  0.142857\n",
       " 0.833333  0.428571\n",
       " 1.0       0.571429\n",
       " 1.0       0.142857\n",
       " 1.0       0.571429\n",
       " 0.333333  0.428571\n",
       " 0.5       0.428571\n",
       " 1.0       0.428571\n",
       " ⋮         \n",
       " 0.333333  0.428571\n",
       " 0.333333  0.142857\n",
       " 0.5       0.142857\n",
       " 0.5       0.857143\n",
       " 0.166667  0.571429\n",
       " 0.166667  0.142857\n",
       " 0.333333  0.285714\n",
       " 0.666667  0.285714\n",
       " 0.0       0.428571\n",
       " 0.166667  0.142857\n",
       " 0.0       0.857143\n",
       " 0.333333  0.142857"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize ordinal variables (min-max)\n",
    "\n",
    "ordinal_columns = categorical_columns[categorical_columns .!= :cut]\n",
    "\n",
    "normalization_parameters = getMinMaxNormalizationParameters(train_input, ordinal_columns) # Only with train set.\n",
    "\n",
    "train_input_ordinal_columns_minMax  = getMinMaxNormalizedColumns(train_input, ordinal_columns, normalization_parameters)\n",
    "test_input_ordinal_columns_minMax   = getMinMaxNormalizedColumns(test_input,  ordinal_columns, normalization_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4739×2 Matrix{Float64}:\n",
       " 1.0       0.428571\n",
       " 0.666667  0.857143\n",
       " 0.5       0.714286\n",
       " 0.5       0.714286\n",
       " 0.833333  0.285714\n",
       " 0.333333  0.142857\n",
       " 0.833333  0.428571\n",
       " 1.0       0.571429\n",
       " 1.0       0.142857\n",
       " 1.0       0.571429\n",
       " 0.333333  0.428571\n",
       " 0.5       0.428571\n",
       " 1.0       0.428571\n",
       " ⋮         \n",
       " 0.333333  0.428571\n",
       " 0.333333  0.142857\n",
       " 0.5       0.142857\n",
       " 0.5       0.857143\n",
       " 0.166667  0.571429\n",
       " 0.166667  0.142857\n",
       " 0.333333  0.285714\n",
       " 0.666667  0.285714\n",
       " 0.0       0.428571\n",
       " 0.166667  0.142857\n",
       " 0.0       0.857143\n",
       " 0.333333  0.142857"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input_transformed = Matrix{Float64}(undef, size(train_input))\n",
    "test_input_transformed  = Matrix{Float64}(undef, size(test_input))\n",
    "\n",
    "train_input_transformed[:, numerical_indices]   = train_input_numerical_columns_zeroMean\n",
    "train_input_transformed[:, ordinal_indices]     = train_input_ordinal_columns_minMax\n",
    "\n",
    "test_input_transformed[:, numerical_indices]   = test_input_numerical_columns_zeroMean\n",
    "test_input_transformed[:, ordinal_indices]     = test_input_ordinal_columns_minMax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Perform cross-validation for selecting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4739-element Vector{String15}:\n",
       " \"Ideal\"\n",
       " \"Ideal\"\n",
       " \"Very Good\"\n",
       " \"Premium\"\n",
       " \"Very Good\"\n",
       " \"Premium\"\n",
       " \"Premium\"\n",
       " \"Ideal\"\n",
       " \"Very Good\"\n",
       " \"Very Good\"\n",
       " \"Ideal\"\n",
       " \"Very Good\"\n",
       " \"Premium\"\n",
       " ⋮\n",
       " \"Premium\"\n",
       " \"Very Good\"\n",
       " \"Good\"\n",
       " \"Premium\"\n",
       " \"Premium\"\n",
       " \"Good\"\n",
       " \"Premium\"\n",
       " \"Premium\"\n",
       " \"Good\"\n",
       " \"Ideal\"\n",
       " \"Premium\"\n",
       " \"Premium\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "train_inputs = train_input_transformed\n",
    "train_targets = train_output[:,1]\n",
    "\n",
    "kfoldindex = crossvalidation(train_targets, 5)\n",
    "\n",
    "# Test\n",
    "test_inputs = test_input_transformed\n",
    "test_targets = test_output[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────────┬──────────┬─────────────┬─────────────┬────────┬────────┬────────┐\n",
      "│\u001b[1m              SVM \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m    PPV \u001b[0m│\u001b[1m    NPV \u001b[0m│\u001b[1m Fscore \u001b[0m│\n",
      "├──────────────────┼──────────┼─────────────┼─────────────┼────────┼────────┼────────┤\n",
      "│ C=10, kernel=rbf │   0.7787 │      0.7787 │       0.894 │ 0.7766 │ 0.9234 │ 0.7738 │\n",
      "└──────────────────┴──────────┴─────────────┴─────────────┴────────┴────────┴────────┘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 4 entries:\n",
       "  \"C\"      => 10\n",
       "  \"kernel\" => \"rbf\"\n",
       "  \"gamma\"  => \"scale\"\n",
       "  \"degree\" => 3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM classifier\n",
    "\n",
    "# 8 different models varing C and kernel\n",
    "parameters = [\n",
    "    Dict(\"kernel\" => \"rbf\",     \"C\" => 0.1,),\n",
    "    Dict(\"kernel\" => \"poly\",    \"C\" => 0.1,),\n",
    "    Dict(\"kernel\" => \"sigmoid\", \"C\" => 0.1,),\n",
    "    Dict(\"kernel\" => \"rbf\",     \"C\" => 1,),\n",
    "    Dict(\"kernel\" => \"poly\",    \"C\" => 1,),\n",
    "    Dict(\"kernel\" => \"sigmoid\", \"C\" => 1,),\n",
    "    Dict(\"kernel\" => \"rbf\",     \"C\" => 10,),\n",
    "    Dict(\"kernel\" => \"poly\",    \"C\" => 10,),\n",
    "    Dict(\"kernel\" => \"sigmoid\", \"C\" => 10,),\n",
    "];\n",
    "\n",
    "common_parameters = Dict(\"degree\" => 3, \"gamma\" => \"scale\")\n",
    "\n",
    "println(\"Cross-validating SVM classifier...\")\n",
    "\n",
    "models_performance = performCrossValidationTests(parameters, common_parameters, :SVM, train_inputs, train_targets, kfoldindex)\n",
    "\n",
    "svm_best_model_parameters = getBestModelParameters(\"Fscore\", models_performance, parameters, common_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COnfussionMatrix: (0.8656978788233916, 0.13430212117660847, 0.9173304037352376, 0.8272690106295993, 0.7980884109916367, 0.9307727690892365, 0.8535650396115512, [4047 845; 301 3340])\n",
      "1 class ConfusionMatrix sensitivity0.9173304037352376\n",
      "COnfussionMatrix: (0.8363998593695067, 0.16360014063049338, 0.8444954128440367, 0.8336219109082323, 0.6352657004830918, 0.9398402839396628, 0.7250886175659709, [5296 1057; 339 1841])\n",
      "1 class ConfusionMatrix sensitivity0.8444954128440367\n",
      "COnfussionMatrix: (0.9514824797843666, 0.04851752021563342, 0.5416666666666666, 0.9865157104694059, 0.774468085106383, 0.9618008185538881, 0.637478108581436, [7755 106; 308 364])\n",
      "1 class ConfusionMatrix sensitivity0.5416666666666666\n",
      "COnfussionMatrix: (0.8014766201804758, 0.1985233798195242, 0.30656565656565654, 0.951014802380589, 0.6540948275862069, 0.8194608809993426, 0.41746905089408526, [6232 321; 1373 607])\n",
      "1 class ConfusionMatrix sensitivity0.30656565656565654\n",
      "COnfussionMatrix: (0.9962498535099027, 0.0037501464900972694, 0.6666666666666666, 0.998583736575003, 0.7692307692307693, 0.9976417875250561, 0.7142857142857142, [8461 12; 20 40])\n",
      "1 class ConfusionMatrix sensitivity0.6666666666666666\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7256533458338217\n",
      "outerConfusionMatrix(0.7256533458338217, 0.2743466541661783, 0.7256533458338217, 0.871351825129008, 0.7210152688901574, 0.9101742262909631, 0.7015533397531504, [3340 215 5 81 0; 265 1841 0 74 0; 17 118 364 163 10; 560 716 95 607 2; 3 8 6 3 40])\n",
      "COnfussionMatrix: (0.8770511017346461, 0.12294889826535396, 0.9203515517714913, 0.8448170108362298, 0.8153284671532847, 0.9344188150158299, 0.8646626241775255, [4132 759; 290 3351])\n",
      "1 class ConfusionMatrix sensitivity0.9203515517714913\n",
      "COnfussionMatrix: (0.7991092358180966, 0.20089076418190344, 0.314300151591713, 0.9455211353578513, 0.6353421859039836, 0.8203362902158083, 0.4205544286680189, [6196 357; 1357 622])\n",
      "1 class ConfusionMatrix sensitivity0.314300151591713\n",
      "COnfussionMatrix: (0.8379043600562588, 0.1620956399437412, 0.8555045871559633, 0.8318639798488665, 0.6358677122400272, 0.9437399535631362, 0.7295130060629768, [5284 1068; 315 1865])\n",
      "1 class ConfusionMatrix sensitivity0.8555045871559633\n",
      "COnfussionMatrix: (0.953469292076887, 0.046530707923112986, 0.5461309523809523, 0.988295165394402, 0.7995642701525054, 0.9622197448284405, 0.6489832007073386, [7768 92; 305 367])\n",
      "1 class ConfusionMatrix sensitivity0.5461309523809523\n",
      "COnfussionMatrix: (0.9961322081575246, 0.003867791842475387, 0.65, 0.9985835694050992, 0.7647058823529411, 0.9975238769013088, 0.7027027027027027, [8460 12; 21 39])\n",
      "1 class ConfusionMatrix sensitivity0.65\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7318330989217066\n",
      "outerConfusionMatrix(0.7318330989217066, 0.26816690107829344, 0.7318330989217066, 0.8772477619207422, 0.7261291947932504, 0.9129724125311942, 0.7089933257406426, [3351 112 173 5 0; 496 622 776 81 4; 247 68 1865 0 0; 13 173 111 367 8; 3 4 8 6 39])\n",
      "COnfussionMatrix: (0.8012895662368112, 0.19871043376318875, 0.27791814047498736, 0.9593955121355519, 0.6740196078431373, 0.8147523982369718, 0.3935599284436494, [6285 266; 1429 550])\n",
      "1 class ConfusionMatrix sensitivity0.27791814047498736\n",
      "COnfussionMatrix: (0.8368112543962485, 0.16318874560375146, 0.872418540614961, 0.824594552039049, 0.630514096185738, 0.9495920217588395, 0.731998459761263, [5237 1114; 278 1901])\n",
      "1 class ConfusionMatrix sensitivity0.872418540614961\n",
      "COnfussionMatrix: (0.8783118405627198, 0.12168815943728019, 0.9296896457017303, 0.8400490897934138, 0.8123350131989441, 0.9413247765299106, 0.8670594262295083, [4107 782; 256 3385])\n",
      "1 class ConfusionMatrix sensitivity0.9296896457017303\n",
      "COnfussionMatrix: (0.9566236811254396, 0.04337631887456037, 0.5752608047690015, 0.9891843746023667, 0.8195329087048833, 0.9646358108946519, 0.6760070052539405, [7774 85; 285 386])\n",
      "1 class ConfusionMatrix sensitivity0.5752608047690015\n",
      "COnfussionMatrix: (0.9966002344665885, 0.003399765533411489, 0.7666666666666667, 0.9982290436835891, 0.7540983606557377, 0.9983469122682725, 0.7603305785123967, [8455 15; 14 46])\n",
      "1 class ConfusionMatrix sensitivity0.7666666666666667\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7348182883939038\n",
      "outerConfusionMatrix(0.7348182883939038, 0.26518171160609616, 0.7348182883939038, 0.8766342838600666, 0.7339553676375108, 0.9163060922292252, 0.7069242260665359, [550 823 526 79 1; 37 1901 241 0 0; 76 177 3385 3 0; 150 109 12 386 14; 3 5 3 3 46])\n",
      "COnfussionMatrix: (0.8065196998123827, 0.19348030018761725, 0.29711975745325925, 0.96045197740113, 0.6942148760330579, 0.8189037885691968, 0.416135881104034, [6290 259; 1391 588])\n",
      "1 class ConfusionMatrix sensitivity0.29711975745325925\n",
      "COnfussionMatrix: (0.875703564727955, 0.12429643527204502, 0.9241758241758242, 0.839607201309329, 0.81099324975892, 0.936986301369863, 0.8638931689779148, [4104 784; 276 3364])\n",
      "1 class ConfusionMatrix sensitivity0.9241758241758242\n",
      "COnfussionMatrix: (0.837828330206379, 0.16217166979362102, 0.8678292794860027, 0.8275318947865805, 0.6332886805090422, 0.9480332010104655, 0.7322362052274927, [5254 1095; 288 1891])\n",
      "1 class ConfusionMatrix sensitivity0.8678292794860027\n",
      "COnfussionMatrix: (0.9550891181988743, 0.0449108818011257, 0.5767511177347243, 0.9873997709049256, 0.7962962962962963, 0.9646854016413827, 0.6689714779602419, [7758 99; 284 387])\n",
      "1 class ConfusionMatrix sensitivity0.5767511177347243\n",
      "COnfussionMatrix: (0.9962476547842402, 0.00375234521575985, 0.7457627118644068, 0.9979926791829024, 0.7213114754098361, 0.9982284162040864, 0.7333333333333334, [8452 17; 15 44])\n",
      "1 class ConfusionMatrix sensitivity0.7457627118644068\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7356941838649156\n",
      "outerConfusionMatrix(0.7356941838649156, 0.2643058161350844, 0.7356941838649156, 0.8772893397199518, 0.7367114089452496, 0.9150099039193194, 0.7101070899804396, [588 515 784 91 1; 85 3364 186 4 1; 34 254 1891 0 0; 139 14 116 387 15; 1 1 9 4 44])\n",
      "COnfussionMatrix: (0.8307926829268293, 0.16920731707317074, 0.8650757228086278, 0.8190266183650968, 0.6212920237310481, 0.9464870768110666, 0.7231920199501246, [5200 1149; 294 1885])\n",
      "1 class ConfusionMatrix sensitivity0.8650757228086278\n",
      "COnfussionMatrix: (0.8791041275797373, 0.12089587242026266, 0.9280219780219781, 0.8426759410801964, 0.8145647455992283, 0.9401963022141063, 0.8675998458970078, [4119 769; 262 3378])\n",
      "1 class ConfusionMatrix sensitivity0.9280219780219781\n",
      "COnfussionMatrix: (0.800656660412758, 0.199343339587242, 0.2764022233451238, 0.9590777217895862, 0.6711656441717792, 0.8143394269415273, 0.3915533285612026, [6281 268; 1432 547])\n",
      "1 class ConfusionMatrix sensitivity0.2764022233451238\n",
      "COnfussionMatrix: (0.9532129455909943, 0.04678705440900563, 0.5603576751117735, 0.9867633956981036, 0.7833333333333333, 0.963344930417495, 0.6533449174630755, [7753 104; 295 376])\n",
      "1 class ConfusionMatrix sensitivity0.5603576751117735\n",
      "COnfussionMatrix: (0.9970684803001876, 0.0029315196998123826, 0.7288135593220338, 0.9989373007438895, 0.8269230769230769, 0.998112317130722, 0.7747747747747747, [8460 9; 16 43])\n",
      "1 class ConfusionMatrix sensitivity0.7288135593220338\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7304174484052532\n",
      "outerConfusionMatrix(0.7304174484052532, 0.26958255159474676, 0.7304174484052532, 0.8760635292716192, 0.7295324732307955, 0.9148194982843334, 0.7027308910751039, [1885 245 49 0 0; 188 3378 71 3 0; 836 499 547 94 3; 118 23 148 376 6; 7 2 0 7 43])\n",
      "COnfussionMatrix: (0.8660494550568382, 0.13395054494316183, 0.91128810766273, 0.8323793949304987, 0.8018366360560657, 0.9265073947667805, 0.8530659467797918, [4072 820; 323 3318])\n",
      "1 class ConfusionMatrix sensitivity0.91128810766273\n",
      "COnfussionMatrix: (0.8423766553380991, 0.15762334466190087, 0.8362385321100917, 0.8444829214544309, 0.6485236570615439, 0.9376092275428172, 0.730514926868363, [5365 988; 357 1823])\n",
      "1 class ConfusionMatrix sensitivity0.8362385321100917\n",
      "COnfussionMatrix: (0.9538263213406774, 0.04617367865932263, 0.5833333333333334, 0.9854980282406819, 0.7747035573122529, 0.9651177276691167, 0.66553480475382, [7747 114; 280 392])\n",
      "1 class ConfusionMatrix sensitivity0.5833333333333334\n",
      "COnfussionMatrix: (0.8060471112152818, 0.19395288878471814, 0.3429292929292929, 0.9459789409430795, 0.6573088092933205, 0.8265333333333333, 0.4507135745104547, [6199 354; 1301 679])\n",
      "1 class ConfusionMatrix sensitivity0.3429292929292929\n",
      "COnfussionMatrix: (0.9963670455877183, 0.00363295441228173, 0.6166666666666667, 0.9990558243833353, 0.8222222222222222, 0.9972902921771913, 0.7047619047619048, [8465 8; 23 37])\n",
      "1 class ConfusionMatrix sensitivity0.6166666666666667\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7323332942693074\n",
      "outerConfusionMatrix(0.7323332942693074, 0.2676667057306926, 0.7323332942693074, 0.8750618156827189, 0.727138676962644, 0.9096840584687059, 0.7125833392018323, [3318 211 5 107 0; 263 1823 0 94 0; 17 106 392 150 7; 537 663 100 679 1; 3 8 9 3 37])\n",
      "COnfussionMatrix: (0.8770511017346461, 0.12294889826535396, 0.9299642955232079, 0.8376610100184011, 0.8100478468899521, 0.94140625, 0.8658739291650684, [4097 794; 255 3386])\n",
      "1 class ConfusionMatrix sensitivity0.9299642955232079\n",
      "COnfussionMatrix: (0.809774964838256, 0.19022503516174402, 0.33400707427993936, 0.9534564321684724, 0.6842650103519669, 0.8257996299233412, 0.44889643463497453, [6248 305; 1318 661])\n",
      "1 class ConfusionMatrix sensitivity0.33400707427993936\n",
      "COnfussionMatrix: (0.8442334739803095, 0.15576652601969057, 0.8481651376146789, 0.8428841309823678, 0.6494555672637864, 0.9417766051011434, 0.7356276109011338, [5354 998; 331 1849])\n",
      "1 class ConfusionMatrix sensitivity0.8481651376146789\n",
      "COnfussionMatrix: (0.957571495546179, 0.04242850445382091, 0.59375, 0.988676844783715, 0.8176229508196722, 0.9660616608652411, 0.6879310344827586, [7771 89; 273 399])\n",
      "1 class ConfusionMatrix sensitivity0.59375\n",
      "COnfussionMatrix: (0.996601031411158, 0.0033989685888420064, 0.6833333333333333, 0.9988196411709159, 0.803921568627451, 0.9977596981488032, 0.7387387387387387, [8462 10; 19 41])\n",
      "1 class ConfusionMatrix sensitivity0.6833333333333333\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7426160337552743\n",
      "outerConfusionMatrix(0.7426160337552743, 0.2573839662447257, 0.7426160337552743, 0.878882025368598, 0.7403933098589287, 0.917024102318232, 0.7209675564338769, [3386 76 173 6 0; 516 661 719 79 4; 259 72 1849 0 0; 15 154 98 399 6; 4 3 8 4 41])\n",
      "COnfussionMatrix: (0.8049237983587338, 0.19507620164126613, 0.30672056594239516, 0.955426652419478, 0.675194660734149, 0.8202070501900144, 0.4218207088255733, [6259 292; 1372 607])\n",
      "1 class ConfusionMatrix sensitivity0.30672056594239516\n",
      "COnfussionMatrix: (0.8412661195779602, 0.15873388042203987, 0.8678292794860027, 0.8321524169422139, 0.6394994927291173, 0.9483222680782344, 0.7363707165109035, [5285 1066; 288 1891])\n",
      "1 class ConfusionMatrix sensitivity0.8678292794860027\n",
      "COnfussionMatrix: (0.8797186400937866, 0.12028135990621336, 0.9261191980225213, 0.8451626099406832, 0.8166626301767983, 0.938877527834583, 0.8679536679536679, [4132 757; 269 3372])\n",
      "1 class ConfusionMatrix sensitivity0.9261191980225213\n",
      "COnfussionMatrix: (0.9577960140679953, 0.04220398593200469, 0.6005961251862891, 0.9882936760402087, 0.8141414141414142, 0.9666459240821407, 0.6912521440823328, [7767 92; 268 403])\n",
      "1 class ConfusionMatrix sensitivity0.6005961251862891\n",
      "COnfussionMatrix: (0.9960140679953107, 0.003985932004689332, 0.6333333333333333, 0.9985832349468713, 0.76, 0.9974056603773584, 0.6909090909090909, [8458 12; 22 38])\n",
      "1 class ConfusionMatrix sensitivity0.6333333333333333\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7398593200468934\n",
      "outerConfusionMatrix(0.7398593200468934, 0.26014067995310663, 0.7398593200468934, 0.8797592702425616, 0.7379879429791605, 0.9163541392726829, 0.7156903884169007, [607 787 507 77 1; 53 1891 235 0 0; 88 177 3372 4 0; 148 97 12 403 11; 3 5 3 11 38])\n",
      "COnfussionMatrix: (0.8089821763602252, 0.19101782363977485, 0.3355229914098029, 0.9520537486639181, 0.6789366053169734, 0.8258278145695365, 0.44910382144064925, [6235 314; 1315 664])\n",
      "1 class ConfusionMatrix sensitivity0.3355229914098029\n",
      "COnfussionMatrix: (0.8773452157598499, 0.1226547842401501, 0.9156593406593406, 0.8488134206219312, 0.818516699410609, 0.9311041292639138, 0.8643672199170124, [4149 739; 307 3333])\n",
      "1 class ConfusionMatrix sensitivity0.9156593406593406\n",
      "COnfussionMatrix: (0.8421669793621013, 0.1578330206378987, 0.8604864616796696, 0.8358796660891479, 0.6427836818649297, 0.9458207093209766, 0.7358712715855573, [5307 1042; 304 1875])\n",
      "1 class ConfusionMatrix sensitivity0.8604864616796696\n",
      "COnfussionMatrix: (0.9571998123827392, 0.04280018761726079, 0.6035767511177347, 0.9873997709049256, 0.8035714285714286, 0.9668494516450648, 0.6893617021276595, [7758 99; 266 405])\n",
      "1 class ConfusionMatrix sensitivity0.6035767511177347\n",
      "COnfussionMatrix: (0.9967166979362101, 0.003283302063789869, 0.7457627118644068, 0.9984649899633959, 0.7719298245614035, 0.9982292527446582, 0.7586206896551725, [8456 13; 15 44])\n",
      "1 class ConfusionMatrix sensitivity0.7457627118644068\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7412054409005628\n",
      "outerConfusionMatrix(0.7412054409005628, 0.25879455909943716, 0.7412054409005628, 0.881406155342756, 0.739725874622554, 0.9137109649542613, 0.7206680308080692, [664 480 744 90 1; 115 3333 187 4 1; 58 245 1875 1 0; 140 13 102 405 11; 1 1 9 4 44])\n",
      "COnfussionMatrix: (0.8365384615384616, 0.16346153846153846, 0.8581918311151905, 0.8291069459757442, 0.6328257191201354, 0.9445541001256056, 0.728476821192053, [5264 1085; 309 1870])\n",
      "1 class ConfusionMatrix sensitivity0.8581918311151905\n",
      "COnfussionMatrix: (0.8787523452157598, 0.12124765478424016, 0.9195054945054945, 0.848404255319149, 0.8187377690802349, 0.934009009009009, 0.8662008281573499, [4147 741; 293 3347])\n",
      "1 class ConfusionMatrix sensitivity0.9195054945054945\n",
      "COnfussionMatrix: (0.8000703564727955, 0.1999296435272045, 0.30015159171298633, 0.9511375782562224, 0.649890590809628, 0.8180982400840557, 0.4106463878326996, [6229 320; 1385 594])\n",
      "1 class ConfusionMatrix sensitivity0.30015159171298633\n",
      "COnfussionMatrix: (0.9539165103189493, 0.04608348968105066, 0.5961251862891207, 0.9844724449535446, 0.7662835249042146, 0.9661503872095928, 0.6705783738474435, [7735 122; 271 400])\n",
      "1 class ConfusionMatrix sensitivity0.5961251862891207\n",
      "COnfussionMatrix: (0.9967166979362101, 0.003283302063789869, 0.6779661016949152, 0.9989373007438895, 0.8163265306122449, 0.9977591697134096, 0.7407407407407407, [8460 9; 19 40])\n",
      "1 class ConfusionMatrix sensitivity0.6779661016949152\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7329971857410882\n",
      "outerConfusionMatrix(0.7329971857410882, 0.2670028142589118, 0.7329971857410882, 0.8790613395074615, 0.7279087372354518, 0.9127752456523875, 0.7090352957254324, [1870 240 69 0 0; 187 3347 102 4 0; 793 481 594 108 3; 98 18 149 400 6; 7 2 0 10 40])\n",
      "COnfussionMatrix: (0.8678073362240712, 0.13219266377592875, 0.9126613567701181, 0.8344235486508585, 0.8040164529397532, 0.9277272727272727, 0.8549009518909184, [4082 810; 318 3323])\n",
      "1 class ConfusionMatrix sensitivity0.9126613567701181\n",
      "COnfussionMatrix: (0.8552677838978086, 0.1447322161021915, 0.8032110091743119, 0.8731308043444043, 0.6847868596010951, 0.9282128514056225, 0.7392864682288369, [5547 806; 429 1751])\n",
      "1 class ConfusionMatrix sensitivity0.8032110091743119\n",
      "COnfussionMatrix: (0.955701394585726, 0.044298605414273995, 0.6220238095238095, 0.9842259254547767, 0.7712177121771218, 0.9682142410211488, 0.6886326194398682, [7737 124; 254 418])\n",
      "1 class ConfusionMatrix sensitivity0.6220238095238095\n",
      "COnfussionMatrix: (0.8127270596507676, 0.1872729403492324, 0.4131313131313131, 0.933465588280177, 0.6523125996810207, 0.8403626871823052, 0.5058750773036488, [6117 436; 1162 818])\n",
      "1 class ConfusionMatrix sensitivity0.4131313131313131\n",
      "COnfussionMatrix: (0.9966014297433493, 0.0033985702566506504, 0.65, 0.9990558243833353, 0.8297872340425532, 0.9975253358472779, 0.7289719626168223, [8465 8; 21 39])\n",
      "1 class ConfusionMatrix sensitivity0.65\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7440525020508614\n",
      "outerConfusionMatrix(0.7440525020508614, 0.2559474979491386, 0.7440525020508614, 0.8802491890626905, 0.7359526242913429, 0.9112584819878435, 0.7303962214526573, [3323 186 5 127 0; 268 1751 0 161 0; 18 84 418 145 7; 521 528 112 818 1; 3 8 7 3 39])\n",
      "COnfussionMatrix: (0.8769338959212377, 0.12306610407876231, 0.9247459489151332, 0.8413412390104273, 0.812696113927106, 0.9375712007290955, 0.8651079136690647, [4115 776; 274 3367])\n",
      "1 class ConfusionMatrix sensitivity0.9247459489151332\n",
      "COnfussionMatrix: (0.8047351148616971, 0.19526488513830287, 0.40373926225366347, 0.9258354951930413, 0.6217898832684825, 0.837174003035739, 0.4895833333333333, [6067 486; 1180 799])\n",
      "1 class ConfusionMatrix sensitivity0.40373926225366347\n",
      "COnfussionMatrix: (0.8517346460384435, 0.1482653539615565, 0.781651376146789, 0.8757871536523929, 0.6835138387484958, 0.9211790031462163, 0.7292959554889792, [5563 789; 476 1704])\n",
      "1 class ConfusionMatrix sensitivity0.781651376146789\n",
      "COnfussionMatrix: (0.9546413502109705, 0.04535864978902954, 0.6294642857142857, 0.982442748091603, 0.7540106951871658, 0.9687617613850207, 0.6861313868613139, [7722 138; 249 423])\n",
      "1 class ConfusionMatrix sensitivity0.6294642857142857\n",
      "COnfussionMatrix: (0.9957805907172996, 0.004219409282700422, 0.6166666666666667, 0.9984655335221907, 0.74, 0.9972883753831644, 0.6727272727272727, [8459 13; 23 37])\n",
      "1 class ConfusionMatrix sensitivity0.6166666666666667\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7419127988748242\n",
      "outerConfusionMatrix(0.7419127988748242, 0.2580872011251758, 0.7419127988748242, 0.8819593705948312, 0.7302747404400393, 0.9129722839773605, 0.7278542477304436, [3367 110 155 9 0; 504 799 567 104 5; 250 209 1704 17 0; 17 163 61 423 8; 5 4 6 8 37])\n",
      "COnfussionMatrix: (0.8114888628370457, 0.18851113716295428, 0.358767054067711, 0.9482521752404213, 0.6768350810295519, 0.830370271354097, 0.46895640686922063, [6212 339; 1269 710])\n",
      "1 class ConfusionMatrix sensitivity0.358767054067711\n",
      "COnfussionMatrix: (0.8504103165298945, 0.1495896834701055, 0.8563561266636072, 0.8483703353802551, 0.6595970307529162, 0.9450973513418699, 0.7452076677316293, [5388 963; 313 1866])\n",
      "1 class ConfusionMatrix sensitivity0.8563561266636072\n",
      "COnfussionMatrix: (0.8803048065650645, 0.11969519343493552, 0.9266684976654765, 0.8457762323583555, 0.8173449612403101, 0.9393457519309405, 0.8685802548590553, [4135 754; 267 3374])\n",
      "1 class ConfusionMatrix sensitivity0.9266684976654765\n",
      "COnfussionMatrix: (0.95978898007034, 0.04021101992966002, 0.6035767511177347, 0.9902023158162616, 0.8402489626556017, 0.966948310139165, 0.7025151777970512, [7782 77; 266 405])\n",
      "1 class ConfusionMatrix sensitivity0.6035767511177347\n",
      "COnfussionMatrix: (0.9960140679953107, 0.003985932004689332, 0.5666666666666667, 0.9990554899645808, 0.8095238095238095, 0.9969368520263902, 0.6666666666666666, [8462 8; 26 34])\n",
      "1 class ConfusionMatrix sensitivity0.5666666666666667\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7490035169988276\n",
      "outerConfusionMatrix(0.7490035169988276, 0.2509964830011724, 0.7490035169988276, 0.8826530317610467, 0.7461957844967635, 0.9181085938557104, 0.7298664278341207, [710 709 495 64 1; 67 1866 246 0 0; 99 166 3374 2 0; 166 83 10 405 7; 7 5 3 11 34])\n",
      "COnfussionMatrix: (0.8180112570356473, 0.18198874296435272, 0.3916119252147549, 0.9468621163536418, 0.6901157613535174, 0.837407157326131, 0.4996776273372018, [6201 348; 1204 775])\n",
      "1 class ConfusionMatrix sensitivity0.3916119252147549\n",
      "COnfussionMatrix: (0.8806285178236398, 0.11937148217636022, 0.9247252747252748, 0.8477905073649754, 0.8189781021897811, 0.9379809868718877, 0.8686451612903227, [4144 744; 274 3366])\n",
      "1 class ConfusionMatrix sensitivity0.9247252747252748\n",
      "COnfussionMatrix: (0.8524859287054409, 0.1475140712945591, 0.8301973382285452, 0.8601354544022681, 0.6707452725250278, 0.9365460469902247, 0.7420016406890895, [5461 888; 370 1809])\n",
      "1 class ConfusionMatrix sensitivity0.8301973382285452\n",
      "COnfussionMatrix: (0.9579033771106942, 0.04209662288930582, 0.6378539493293591, 0.9852360952017309, 0.7867647058823529, 0.9695641282565131, 0.7045267489711933, [7741 116; 243 428])\n",
      "1 class ConfusionMatrix sensitivity0.6378539493293591\n",
      "COnfussionMatrix: (0.9968339587242027, 0.0031660412757973733, 0.7288135593220338, 0.9987011453536427, 0.7962962962962963, 0.9981118716072693, 0.7610619469026549, [8458 11; 16 43])\n",
      "1 class ConfusionMatrix sensitivity0.7288135593220338\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7529315196998124\n",
      "outerConfusionMatrix(0.7529315196998124, 0.24706848030018758, 0.7529315196998124, 0.8857937989764467, 0.7485077311972314, 0.917176304343191, 0.7370065771676301, [775 476 626 101 1; 94 3366 170 9 1; 110 258 1809 2 0; 141 9 84 428 9; 3 1 8 4 43])\n",
      "COnfussionMatrix: (0.8475609756097561, 0.1524390243902439, 0.8412115649380449, 0.849740116553788, 0.6576964477933261, 0.9397317540498171, 0.738219895287958, [5395 954; 346 1833])\n",
      "1 class ConfusionMatrix sensitivity0.8412115649380449\n",
      "COnfussionMatrix: (0.8779315196998124, 0.12206848030018762, 0.9324175824175824, 0.8373567921440261, 0.8102172356170924, 0.9433049089651994, 0.8670328266700728, [4093 795; 246 3394])\n",
      "1 class ConfusionMatrix sensitivity0.9324175824175824\n",
      "COnfussionMatrix: (0.8105065666041276, 0.1894934333958724, 0.33602829711975746, 0.9538860894793099, 0.687693898655636, 0.826213463827536, 0.451459606245757, [6247 302; 1314 665])\n",
      "1 class ConfusionMatrix sensitivity0.33602829711975746\n",
      "COnfussionMatrix: (0.9535647279549718, 0.04643527204502814, 0.6065573770491803, 0.9831996945399007, 0.7551020408163265, 0.9669545625234698, 0.6727272727272727, [7725 132; 264 407])\n",
      "1 class ConfusionMatrix sensitivity0.6065573770491803\n",
      "COnfussionMatrix: (0.9965994371482176, 0.003400562851782364, 0.6440677966101694, 0.9990553784390128, 0.8260869565217391, 0.9975241688281066, 0.7238095238095238, [8461 8; 21 38])\n",
      "1 class ConfusionMatrix sensitivity0.6440677966101694\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7430816135084428\n",
      "outerConfusionMatrix(0.7430816135084428, 0.2569183864915572, 0.7430816135084428, 0.8801564576475949, 0.7385870218867162, 0.9174557039208111, 0.7214029037965102, [1833 261 81 4 0; 172 3394 71 3 0; 685 513 665 113 3; 90 20 149 407 5; 7 1 1 12 38])\n",
      "COnfussionMatrix: (0.8689792570022267, 0.13102074299777336, 0.9074430101620434, 0.8403515944399019, 0.808812729498164, 0.9242356115107914, 0.8552938130986281, [4111 781; 337 3304])\n",
      "1 class ConfusionMatrix sensitivity0.9074430101620434\n",
      "COnfussionMatrix: (0.8578460096097503, 0.14215399039024962, 0.7912844036697247, 0.8806862899417598, 0.6947241240434958, 0.9247933884297521, 0.7398670383873043, [5595 758; 455 1725])\n",
      "1 class ConfusionMatrix sensitivity0.7912844036697247\n",
      "COnfussionMatrix: (0.9519512480956287, 0.048048751904371266, 0.6071428571428571, 0.9814272993257855, 0.7364620938628159, 0.9669131470109036, 0.6655791190864601, [7715 146; 264 408])\n",
      "1 class ConfusionMatrix sensitivity0.6071428571428571\n",
      "COnfussionMatrix: (0.8114379467947966, 0.18856205320520333, 0.4393939393939394, 0.9238516709903861, 0.635500365230095, 0.8450586264656617, 0.5195580770379218, [6054 499; 1110 870])\n",
      "1 class ConfusionMatrix sensitivity0.4393939393939394\n",
      "COnfussionMatrix: (0.9962498535099027, 0.0037501464900972694, 0.5833333333333334, 0.9991738463354184, 0.8333333333333334, 0.9970557060416912, 0.6862745098039216, [8466 7; 25 35])\n",
      "1 class ConfusionMatrix sensitivity0.5833333333333334\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7432321575061526\n",
      "outerConfusionMatrix(0.7432321575061526, 0.25676784249384743, 0.7432321575061526, 0.8822585435270992, 0.7339246441754408, 0.9098788826723883, 0.7317714225156275, [3304 184 10 142 1; 261 1725 5 189 0; 18 77 408 164 5; 499 490 120 870 1; 3 7 11 4 35])\n",
      "COnfussionMatrix: (0.8768166901078294, 0.12318330989217065, 0.9203515517714913, 0.8444080965037825, 0.814931906614786, 0.9343891402714932, 0.8644395717786664, [4130 761; 290 3351])\n",
      "1 class ConfusionMatrix sensitivity0.9203515517714913\n",
      "COnfussionMatrix: (0.8088373183309893, 0.1911626816690108, 0.4224355735219808, 0.9255302914695559, 0.6314199395770392, 0.8414261931187569, 0.5062064789585226, [6065 488; 1143 836])\n",
      "1 class ConfusionMatrix sensitivity0.4224355735219808\n",
      "COnfussionMatrix: (0.8586497890295358, 0.14135021097046413, 0.7926605504587156, 0.8812972292191436, 0.6962127316680097, 0.9252892561983471, 0.7413127413127413, [5598 754; 452 1728])\n",
      "1 class ConfusionMatrix sensitivity0.7926605504587156\n",
      "COnfussionMatrix: (0.9553445850914205, 0.044655414908579466, 0.6354166666666666, 0.9826972010178117, 0.7584369449378331, 0.9692558664826202, 0.691497975708502, [7724 136; 245 427])\n",
      "1 class ConfusionMatrix sensitivity0.6354166666666666\n",
      "COnfussionMatrix: (0.9956633849038913, 0.004336615096108767, 0.6166666666666667, 0.9983474976392823, 0.7254901960784313, 0.9972880556538144, 0.6666666666666666, [8458 14; 23 37])\n",
      "1 class ConfusionMatrix sensitivity0.6166666666666667\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7476558837318331\n",
      "outerConfusionMatrix(0.7476558837318331, 0.2523441162681669, 0.7476558837318331, 0.8846244321177429, 0.7369538122605002, 0.9136897679369987, 0.7348752365732454, [3351 130 151 9 0; 482 836 548 108 5; 255 186 1728 11 0; 19 168 49 427 9; 5 4 6 8 37])\n",
      "COnfussionMatrix: (0.8106682297772567, 0.18933177022274325, 0.4017180394138454, 0.9342085177835445, 0.6484502446982056, 0.8378970427163198, 0.4960998439937597, [6120 431; 1184 795])\n",
      "1 class ConfusionMatrix sensitivity0.4017180394138454\n",
      "COnfussionMatrix: (0.856506447831184, 0.14349355216881593, 0.8269848554382745, 0.8666351755629035, 0.6802567006417516, 0.935895255908859, 0.7464788732394366, [5504 847; 377 1802])\n",
      "1 class ConfusionMatrix sensitivity0.8269848554382745\n",
      "COnfussionMatrix: (0.8790152403282532, 0.12098475967174678, 0.9178797033781928, 0.8500715892820617, 0.8201226993865031, 0.9328843995510662, 0.8662519440124417, [4156 733; 299 3342])\n",
      "1 class ConfusionMatrix sensitivity0.9178797033781928\n",
      "COnfussionMatrix: (0.9562719812426729, 0.04372801875732708, 0.6184798807749627, 0.9851126097467872, 0.7800751879699248, 0.9679919979994999, 0.6899418121363259, [7742 117; 256 415])\n",
      "1 class ConfusionMatrix sensitivity0.6184798807749627\n",
      "COnfussionMatrix: (0.9950762016412661, 0.00492379835873388, 0.55, 0.9982290436835891, 0.6875, 0.9968167884932799, 0.6111111111111112, [8455 15; 27 33])\n",
      "1 class ConfusionMatrix sensitivity0.55\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7487690504103165\n",
      "outerConfusionMatrix(0.7487690504103165, 0.2512309495896835, 0.7487690504103165, 0.8854878856485695, 0.740481780134842, 0.9148274029183696, 0.734114889426943, [795 611 478 94 1; 127 1802 240 6 4; 132 162 3342 5 0; 166 69 11 415 10; 6 5 4 12 33])\n",
      "COnfussionMatrix: (0.8177767354596623, 0.18222326454033771, 0.41384537645275393, 0.9398381432279738, 0.6751854905193735, 0.8414217361585783, 0.5131578947368421, [6155 394; 1160 819])\n",
      "1 class ConfusionMatrix sensitivity0.41384537645275393\n",
      "COnfussionMatrix: (0.8780487804878049, 0.12195121951219512, 0.9129120879120879, 0.8520867430441899, 0.8213049925852695, 0.9292726461401161, 0.8646890450169139, [4165 723; 317 3323])\n",
      "1 class ConfusionMatrix sensitivity0.9129120879120879\n",
      "COnfussionMatrix: (0.8590525328330206, 0.14094746716697937, 0.8329508949059201, 0.8680107103480863, 0.6841311722578214, 0.9380425531914893, 0.7512417218543046, [5511 838; 364 1815])\n",
      "1 class ConfusionMatrix sensitivity0.8329508949059201\n",
      "COnfussionMatrix: (0.9547373358348968, 0.04526266416510319, 0.6318926974664679, 0.98230876925035, 0.7531083481349912, 0.9689893283113622, 0.6871961102106969, [7718 139; 247 424])\n",
      "1 class ConfusionMatrix sensitivity0.6318926974664679\n",
      "COnfussionMatrix: (0.9964821763602252, 0.003517823639774859, 0.6949152542372882, 0.9985830676585193, 0.7735849056603774, 0.9978761061946902, 0.7321428571428573, [8457 12; 18 41])\n",
      "1 class ConfusionMatrix sensitivity0.6949152542372882\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7530487804878049\n",
      "outerConfusionMatrix(0.7530487804878049, 0.24695121951219512, 0.7530487804878049, 0.8877786530413143, 0.746651183645703, 0.91472646585067, 0.7392434718561344, [819 465 575 119 1; 126 3323 176 14 1; 114 248 1815 2 0; 150 9 78 424 10; 4 1 9 4 41])\n",
      "COnfussionMatrix: (0.8572936210131332, 0.1427063789868668, 0.8109224414869206, 0.8732083792723263, 0.6870139968895801, 0.9308260577568838, 0.7438434013891813, [5544 805; 412 1767])\n",
      "1 class ConfusionMatrix sensitivity0.8109224414869206\n",
      "COnfussionMatrix: (0.8801594746716698, 0.1198405253283302, 0.9258241758241759, 0.8461538461538461, 0.8175642891800097, 0.9387199273717658, 0.8683329038907498, [4136 752; 270 3370])\n",
      "1 class ConfusionMatrix sensitivity0.9258241758241759\n",
      "COnfussionMatrix: (0.8148452157598499, 0.18515478424015008, 0.39262253663466395, 0.9424339593831119, 0.6733102253032929, 0.8369948467588826, 0.49601021385253746, [6172 377; 1202 777])\n",
      "1 class ConfusionMatrix sensitivity0.39262253663466395\n",
      "COnfussionMatrix: (0.9499296435272045, 0.0500703564727955, 0.6527570789865872, 0.9753086419753086, 0.6930379746835443, 0.9704913880445796, 0.6722947045280123, [7663 194; 233 438])\n",
      "1 class ConfusionMatrix sensitivity0.6527570789865872\n",
      "COnfussionMatrix: (0.9963649155722326, 0.0036350844277673548, 0.6440677966101694, 0.9988192230487661, 0.7916666666666666, 0.9975235849056604, 0.7102803738317757, [8459 10; 21 38])\n",
      "1 class ConfusionMatrix sensitivity0.6440677966101694\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.749296435272045\n",
      "outerConfusionMatrix(0.749296435272045, 0.25070356472795496, 0.749296435272045, 0.8866276145613141, 0.7407546038999576, 0.9160033923673374, 0.7336054225841813, [1767 250 132 28 2; 172 3370 92 6 0; 570 480 777 149 3; 57 21 150 438 5; 6 1 3 11 38])\n",
      "COnfussionMatrix: (0.8679245283018868, 0.1320754716981132, 0.8948091183740731, 0.8479149632052331, 0.8140929535232384, 0.9154711984109468, 0.8525448122465001, [4148 744; 383 3258])\n",
      "1 class ConfusionMatrix sensitivity0.8948091183740731\n",
      "COnfussionMatrix: (0.8584319699988281, 0.1415680300011719, 0.7798165137614679, 0.885408468440107, 0.700164744645799, 0.9213759213759214, 0.7378472222222222, [5625 728; 480 1700])\n",
      "1 class ConfusionMatrix sensitivity0.7798165137614679\n",
      "COnfussionMatrix: (0.949841790694949, 0.05015820930505098, 0.6235119047619048, 0.9777382012466608, 0.7053872053872053, 0.9681320065499434, 0.6619273301737756, [7686 175; 253 419])\n",
      "1 class ConfusionMatrix sensitivity0.6235119047619048\n",
      "COnfussionMatrix: (0.8101488339388258, 0.18985116606117428, 0.4595959595959596, 0.9160689760415077, 0.6232876712328768, 0.8487204863565673, 0.5290697674418605, [6003 550; 1070 910])\n",
      "1 class ConfusionMatrix sensitivity0.4595959595959596\n",
      "COnfussionMatrix: (0.9961326614320872, 0.003867338567912809, 0.6333333333333333, 0.9987017585270861, 0.7755102040816326, 0.9974068835454974, 0.6972477064220183, [8462 11; 22 38])\n",
      "1 class ConfusionMatrix sensitivity0.6333333333333333\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7412398921832885\n",
      "outerConfusionMatrix(0.7412398921832885, 0.25876010781671155, 0.7412398921832885, 0.8845924752773062, 0.7318799941886968, 0.90621420677535, 0.7320790781238641, [3258 203 10 169 1; 254 1700 20 205 1; 21 54 419 170 8; 467 464 138 910 1; 2 7 7 6 38])\n",
      "COnfussionMatrix: (0.8756446319737459, 0.1243553680262541, 0.9107388080197748, 0.8495195256593744, 0.8183613030602171, 0.9274553571428571, 0.8620824125828676, [4155 736; 325 3316])\n",
      "1 class ConfusionMatrix sensitivity0.9107388080197748\n",
      "COnfussionMatrix: (0.8084857008907642, 0.1915142991092358, 0.4477008590197069, 0.9174423927971921, 0.6208829712683952, 0.846164672765658, 0.5202583675866118, [6012 541; 1093 886])\n",
      "1 class ConfusionMatrix sensitivity0.4477008590197069\n",
      "COnfussionMatrix: (0.8621659634317862, 0.13783403656821377, 0.7889908256880734, 0.8872795969773299, 0.7060755336617406, 0.9245406824146981, 0.7452339688041596, [5636 716; 460 1720])\n",
      "1 class ConfusionMatrix sensitivity0.7889908256880734\n",
      "COnfussionMatrix: (0.9517112048757619, 0.048288795124238164, 0.6116071428571429, 0.9807888040712468, 0.7313167259786477, 0.9672521957340026, 0.6661264181523501, [7709 151; 261 411])\n",
      "1 class ConfusionMatrix sensitivity0.6116071428571429\n",
      "COnfussionMatrix: (0.9951945616502579, 0.004805438349742147, 0.6166666666666667, 0.9978753541076487, 0.6727272727272727, 0.9972867759820692, 0.6434782608695652, [8454 18; 23 37])\n",
      "1 class ConfusionMatrix sensitivity0.6166666666666667\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.746601031411158\n",
      "outerConfusionMatrix(0.746601031411158, 0.25339896858884203, 0.746601031411158, 0.886304642201634, 0.7359861748929075, 0.9114808031547814, 0.7359691835808423, [3316 158 157 8 2; 458 886 506 124 5; 255 192 1720 11 2; 19 187 46 411 9; 4 4 7 8 37])\n",
      "COnfussionMatrix: (0.8093786635404455, 0.1906213364595545, 0.45073269327943405, 0.917722485116776, 0.623340321453529, 0.8468798422313002, 0.52316715542522, [6012 539; 1087 892])\n",
      "1 class ConfusionMatrix sensitivity0.45073269327943405\n",
      "COnfussionMatrix: (0.8634232121922626, 0.1365767878077374, 0.8132170720513997, 0.8806487167375217, 0.700395256916996, 0.9321666666666667, 0.7526014015714588, [5593 758; 407 1772])\n",
      "1 class ConfusionMatrix sensitivity0.8132170720513997\n",
      "COnfussionMatrix: (0.8765533411488863, 0.12344665885111372, 0.8994781653391926, 0.8594804663530374, 0.8266027258960121, 0.919877408056042, 0.8615020386689465, [4202 687; 366 3275])\n",
      "1 class ConfusionMatrix sensitivity0.8994781653391926\n",
      "COnfussionMatrix: (0.9518171160609613, 0.04818288393903869, 0.6065573770491803, 0.9812953301946813, 0.7346570397111913, 0.966900702106319, 0.6644897959183673, [7712 147; 264 407])\n",
      "1 class ConfusionMatrix sensitivity0.6065573770491803\n",
      "COnfussionMatrix: (0.9942555685814771, 0.005744431418522861, 0.5333333333333333, 0.9975206611570248, 0.6037735849056604, 0.9966969446738233, 0.5663716814159292, [8449 21; 28 32])\n",
      "1 class ConfusionMatrix sensitivity0.5333333333333333\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7477139507620164\n",
      "outerConfusionMatrix(0.7477139507620164, 0.2522860492379836, 0.7477139507620164, 0.8889537087970248, 0.738404873939812, 0.9103203053890068, 0.7376143178605549, [892 520 447 118 2; 174 1772 222 7 4; 186 170 3275 10 0; 172 64 13 407 15; 7 4 5 12 32])\n",
      "COnfussionMatrix: (0.8130863039399625, 0.18691369606003752, 0.43456291056088936, 0.9274698427240801, 0.6441947565543071, 0.8444320867510079, 0.5190102595051298, [6074 475; 1119 860])\n",
      "1 class ConfusionMatrix sensitivity0.43456291056088936\n",
      "COnfussionMatrix: (0.87406191369606, 0.12593808630393996, 0.8983516483516484, 0.855973813420622, 0.8228485153497735, 0.9187527448397014, 0.8589440504334122, [4184 704; 370 3270])\n",
      "1 class ConfusionMatrix sensitivity0.8983516483516484\n",
      "COnfussionMatrix: (0.8603424015009381, 0.13965759849906192, 0.817806333180358, 0.8749409355804063, 0.6917701863354038, 0.9332997311827957, 0.7495268138801262, [5555 794; 397 1782])\n",
      "1 class ConfusionMatrix sensitivity0.817806333180358\n",
      "COnfussionMatrix: (0.9532129455909943, 0.04678705440900563, 0.6408345752608048, 0.9798905434644266, 0.7312925170068028, 0.9696473551637279, 0.6830818109610802, [7699 158; 241 430])\n",
      "1 class ConfusionMatrix sensitivity0.6408345752608048\n",
      "COnfussionMatrix: (0.9964821763602252, 0.003517823639774859, 0.711864406779661, 0.9984649899633959, 0.7636363636363637, 0.9979936268145875, 0.736842105263158, [8456 13; 17 42])\n",
      "1 class ConfusionMatrix sensitivity0.711864406779661\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7485928705440901\n",
      "outerConfusionMatrix(0.7485928705440901, 0.25140712945590993, 0.7485928705440901, 0.8881472546088407, 0.7402848357745196, 0.9097755868246941, 0.737420046194709, [860 455 533 130 1; 173 3270 179 16 2; 151 238 1782 8 0; 147 10 74 430 10; 4 1 8 4 42])\n",
      "COnfussionMatrix: (0.8665572232645403, 0.13344277673545965, 0.7921064708581919, 0.8921089935422901, 0.7158855246785566, 0.9259440902403139, 0.7520697167755992, [5664 685; 453 1726])\n",
      "1 class ConfusionMatrix sensitivity0.7921064708581919\n",
      "COnfussionMatrix: (0.8754690431519699, 0.12453095684803002, 0.9082417582417582, 0.851063829787234, 0.819533961328706, 0.9256786826880284, 0.8616106333072713, [4160 728; 334 3306])\n",
      "1 class ConfusionMatrix sensitivity0.9082417582417582\n",
      "COnfussionMatrix: (0.8112101313320825, 0.18878986866791744, 0.45022738756947955, 0.9202931745304627, 0.6305732484076433, 0.8470836261419536, 0.5253537735849056, [6027 522; 1088 891])\n",
      "1 class ConfusionMatrix sensitivity0.45022738756947955\n",
      "COnfussionMatrix: (0.9489915572232646, 0.05100844277673546, 0.639344262295082, 0.975435917016673, 0.6897106109324759, 0.9693903364533266, 0.6635730858468678, [7664 193; 242 429])\n",
      "1 class ConfusionMatrix sensitivity0.639344262295082\n",
      "COnfussionMatrix: (0.9961303939962477, 0.003869606003752345, 0.6271186440677966, 0.9987011453536427, 0.7708333333333334, 0.9974056603773584, 0.6915887850467289, [8458 11; 22 37])\n",
      "1 class ConfusionMatrix sensitivity0.6271186440677966\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7491791744840526\n",
      "outerConfusionMatrix(0.7491791744840526, 0.2508208255159474, 0.7491791744840526, 0.8884238857462501, 0.7386488769596798, 0.9114433540660695, 0.7388319670424521, [1726 237 194 20 2; 173 3306 156 5 0; 458 469 891 157 4; 48 22 167 429 5; 6 0 5 11 37])\n",
      "COnfussionMatrix: (0.8586663541544591, 0.14133364584554084, 0.8843724251579237, 0.8395339329517579, 0.8039950062421972, 0.9070229681978799, 0.8422704682186765, [4107 785; 421 3220])\n",
      "1 class ConfusionMatrix sensitivity0.8843724251579237\n",
      "COnfussionMatrix: (0.8630024610336341, 0.13699753896636588, 0.7591743119266054, 0.8986305682354793, 0.7198782079164854, 0.9157844080846969, 0.7390042420183076, [5709 644; 525 1655])\n",
      "1 class ConfusionMatrix sensitivity0.7591743119266054\n",
      "COnfussionMatrix: (0.9470291808273761, 0.05297081917262393, 0.6264880952380952, 0.9744307340033075, 0.6768488745980707, 0.9682720262925041, 0.6506955177743432, [7660 201; 251 421])\n",
      "1 class ConfusionMatrix sensitivity0.6264880952380952\n",
      "COnfussionMatrix: (0.8080393765381461, 0.19196062346185397, 0.47929292929292927, 0.9073706699221731, 0.6098971722365039, 0.8522287516124408, 0.5367647058823529, [5946 607; 1031 949])\n",
      "1 class ConfusionMatrix sensitivity0.47929292929292927\n",
      "COnfussionMatrix: (0.995429508965194, 0.004570491034806047, 0.6, 0.9982296707187537, 0.7058823529411765, 0.9971704786606932, 0.6486486486486486, [8458 15; 24 36])\n",
      "1 class ConfusionMatrix sensitivity0.6\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7360834407594047\n",
      "outerConfusionMatrix(0.7360834407594047, 0.2639165592405953, 0.7360834407594047, 0.8821121350720669, 0.726763400553197, 0.9020042886926002, 0.7285499176015994, [3220 202 15 201 3; 271 1655 29 223 2; 24 40 421 178 9; 488 395 147 949 1; 2 7 10 5 36])\n",
      "COnfussionMatrix: (0.8714252226910455, 0.12857477730895453, 0.8992035155177149, 0.8507462686567164, 0.8176823176823177, 0.9189487632508834, 0.8565075212557227, [4161 730; 367 3274])\n",
      "1 class ConfusionMatrix sensitivity0.8992035155177149\n",
      "COnfussionMatrix: (0.8045007032348804, 0.19549929676511954, 0.4593228903486609, 0.9087440866778574, 0.6031851360318513, 0.8476868327402135, 0.5215146299483648, [5955 598; 1070 909])\n",
      "1 class ConfusionMatrix sensitivity0.4593228903486609\n",
      "COnfussionMatrix: (0.8601734646038444, 0.13982653539615564, 0.768348623853211, 0.8916876574307305, 0.7088446889547185, 0.9181390825093207, 0.7373981950253137, [5664 688; 505 1675])\n",
      "1 class ConfusionMatrix sensitivity0.768348623853211\n",
      "COnfussionMatrix: (0.950070323488045, 0.04992967651195499, 0.6324404761904762, 0.9772264631043257, 0.7036423841059603, 0.9688446014127144, 0.6661442006269592, [7681 179; 247 425])\n",
      "1 class ConfusionMatrix sensitivity0.6324404761904762\n",
      "COnfussionMatrix: (0.9955461790904829, 0.004453820909517112, 0.6333333333333333, 0.9981114258734656, 0.7037037037037037, 0.9974050483604624, 0.6666666666666667, [8456 16; 22 38])\n",
      "1 class ConfusionMatrix sensitivity0.6333333333333333\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.740857946554149\n",
      "outerConfusionMatrix(0.740857946554149, 0.25914205344585095, 0.740857946554149, 0.8856579551889466, 0.7303370873360369, 0.9066942995675854, 0.732043870784857, [3274 186 170 9 2; 459 909 468 138 5; 253 227 1675 23 2; 15 182 43 425 7; 3 3 7 9 38])\n",
      "COnfussionMatrix: (0.8096131301289566, 0.19038686987104336, 0.4689236988377969, 0.9125324377957563, 0.6182544970019986, 0.8504765969554702, 0.5333333333333334, [5978 573; 1051 928])\n",
      "1 class ConfusionMatrix sensitivity0.4689236988377969\n",
      "COnfussionMatrix: (0.8661195779601407, 0.1338804220398593, 0.8012849931161083, 0.8883640371595024, 0.7112016293279022, 0.928724279835391, 0.7535606387570134, [5642 709; 433 1746])\n",
      "1 class ConfusionMatrix sensitivity0.8012849931161083\n",
      "COnfussionMatrix: (0.8720984759671747, 0.12790152403282531, 0.8884921724800879, 0.859889547964819, 0.8252551020408163, 0.9119305856832972, 0.8557069170744611, [4204 685; 406 3235])\n",
      "1 class ConfusionMatrix sensitivity0.8884921724800879\n",
      "COnfussionMatrix: (0.9509964830011723, 0.04900351699882767, 0.639344262295082, 0.9776052932943122, 0.7090909090909091, 0.9694637223974764, 0.6724137931034484, [7683 176; 242 429])\n",
      "1 class ConfusionMatrix sensitivity0.639344262295082\n",
      "COnfussionMatrix: (0.9947245017584995, 0.005275498241500586, 0.5333333333333333, 0.9979929161747344, 0.6530612244897959, 0.9966985025350784, 0.5871559633027523, [8453 17; 28 32])\n",
      "1 class ConfusionMatrix sensitivity0.5333333333333333\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7467760844079718\n",
      "outerConfusionMatrix(0.7467760844079718, 0.25322391559202817, 0.7467760844079718, 0.8896081479811525, 0.7377457795864543, 0.9070849614876776, 0.7385138565717341, [928 477 440 132 2; 183 1746 226 21 3; 216 180 3235 10 0; 167 47 16 429 12; 7 5 3 13 32])\n",
      "COnfussionMatrix: (0.8040572232645403, 0.19594277673545965, 0.47347145022738757, 0.903954802259887, 0.5983397190293742, 0.8503303648376903, 0.5286318758815233, [5920 629; 1042 937])\n",
      "1 class ConfusionMatrix sensitivity0.47347145022738757\n",
      "COnfussionMatrix: (0.8632739212007504, 0.13672607879924953, 0.8700549450549451, 0.8582242225859247, 0.8204663212435234, 0.8986718080548415, 0.8445333333333332, [4195 693; 473 3167])\n",
      "1 class ConfusionMatrix sensitivity0.8700549450549451\n",
      "COnfussionMatrix: (0.8647983114446529, 0.1352016885553471, 0.799449288664525, 0.8872263348558829, 0.708706265256306, 0.928006589785832, 0.751347854216088, [5633 716; 437 1742])\n",
      "1 class ConfusionMatrix sensitivity0.799449288664525\n",
      "COnfussionMatrix: (0.9513367729831145, 0.04866322701688555, 0.6304023845007451, 0.9787450680921471, 0.7169491525423729, 0.9687578735197783, 0.6708961141950832, [7690 167; 248 423])\n",
      "1 class ConfusionMatrix sensitivity0.6304023845007451\n",
      "COnfussionMatrix: (0.9961303939962477, 0.003869606003752345, 0.6779661016949152, 0.9983469122682725, 0.7407407407407407, 0.9977578475336323, 0.7079646017699114, [8455 14; 19 40])\n",
      "1 class ConfusionMatrix sensitivity0.6779661016949152\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7397983114446529\n",
      "outerConfusionMatrix(0.7397983114446529, 0.2602016885553471, 0.7397983114446529, 0.8866990286174613, 0.7316673604993765, 0.9011491297623119, 0.7328086297102545, [937 440 467 133 2; 276 3167 176 19 2; 191 236 1742 9 1; 158 16 65 423 9; 4 1 8 6 40])\n",
      "COnfussionMatrix: (0.8678470919324578, 0.1321529080675422, 0.7985314364387334, 0.891636478185541, 0.7166392092257001, 0.9280327868852459, 0.755372259604949, [5661 688; 439 1740])\n",
      "1 class ConfusionMatrix sensitivity0.7985314364387334\n",
      "COnfussionMatrix: (0.8692542213883677, 0.13074577861163228, 0.8942307692307693, 0.8506546644844517, 0.8168130489335006, 0.9152542372881356, 0.8537704918032787, [4158 730; 385 3255])\n",
      "1 class ConfusionMatrix sensitivity0.8942307692307693\n",
      "COnfussionMatrix: (0.8081613508442776, 0.19183864915572232, 0.44466902475997977, 0.9180027485112231, 0.621030345800988, 0.8454507101673464, 0.5182567726737338, [6012 537; 1099 880])\n",
      "1 class ConfusionMatrix sensitivity0.44466902475997977\n",
      "COnfussionMatrix: (0.9458255159474672, 0.054174484052532834, 0.6378539493293591, 0.972126765941199, 0.6615146831530139, 0.9691663494480396, 0.6494688922610015, [7638 219; 243 428])\n",
      "1 class ConfusionMatrix sensitivity0.6378539493293591\n",
      "COnfussionMatrix: (0.9957786116322702, 0.004221388367729831, 0.6271186440677966, 0.9983469122682725, 0.7254901960784313, 0.9974047422437183, 0.6727272727272727, [8455 14; 22 37])\n",
      "1 class ConfusionMatrix sensitivity0.6271186440677966\n",
      "Llamando a función accuracy de Alejandro\n",
      "Accuracy multiclass: 0.7434333958724203\n",
      "outerConfusionMatrix(0.7434333958724203, 0.2565666041275797, 0.7434333958724203, 0.887334173518267, 0.7329333563936677, 0.907131018063433, 0.733442241177589, [1740 249 174 12 4; 184 3255 195 6 0; 456 454 880 187 2; 42 27 166 428 8; 6 0 2 14 37])\n",
      "┌──────────────┬──────────┬─────────────┬─────────────┬────────┬────────┬────────┐\n",
      "│\u001b[1m DecisionTree \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m    PPV \u001b[0m│\u001b[1m    NPV \u001b[0m│\u001b[1m Fscore \u001b[0m│\n",
      "├──────────────┼──────────┼─────────────┼─────────────┼────────┼────────┼────────┤\n",
      "│  max_depth=5 │   0.7317 │      0.7317 │      0.8757 │ 0.7295 │ 0.9139 │ 0.7061 │\n",
      "│  max_depth=7 │   0.7378 │      0.7378 │      0.8788 │ 0.7346 │ 0.9139 │ 0.7158 │\n",
      "│ max_depth=10 │   0.7462 │      0.7462 │      0.8822 │ 0.7399 │ 0.9154 │ 0.7293 │\n",
      "│ max_depth=12 │   0.7484 │      0.7484 │      0.8854 │ 0.7398 │ 0.9138 │ 0.7347 │\n",
      "│ max_depth=14 │   0.7467 │      0.7467 │      0.8873 │  0.737 │ 0.9098 │ 0.7364 │\n",
      "│ max_depth=16 │   0.7414 │      0.7414 │      0.8863 │ 0.7319 │ 0.9048 │ 0.7331 │\n",
      "└──────────────┴──────────┴─────────────┴─────────────┴────────┴────────┴────────┘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 2 entries:\n",
       "  \"max_depth\"    => 12\n",
       "  \"random_state\" => 318"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DecisionTree classifier\n",
    "\n",
    "# 6 different models varing depth\n",
    "parameters = [\n",
    "    Dict(\"max_depth\" => 5,),\n",
    "    Dict(\"max_depth\" => 7,),\n",
    "    Dict(\"max_depth\" => 10,),\n",
    "    Dict(\"max_depth\" => 12,),\n",
    "    Dict(\"max_depth\" => 14,),\n",
    "    Dict(\"max_depth\" => 16,),\n",
    "];\n",
    "\n",
    "common_parameters = Dict(\"random_state\" => 318)\n",
    "\n",
    "println(\"Cross-validating decision tree classifier...\")\n",
    "\n",
    "models_performance = performCrossValidationTests(parameters, common_parameters, :DecisionTree, train_inputs, train_targets, kfoldindex)\n",
    "\n",
    "dt_best_model_parameters = getBestModelParameters(\"Fscore\", models_performance, parameters, common_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────┬──────────┬─────────────┬─────────────┬────────┬────────┬────────┐\n",
      "│\u001b[1m  kNN \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m    PPV \u001b[0m│\u001b[1m    NPV \u001b[0m│\u001b[1m Fscore \u001b[0m│\n",
      "├──────┼──────────┼─────────────┼─────────────┼────────┼────────┼────────┤\n",
      "│  k=3 │   0.6613 │      0.6613 │      0.8583 │ 0.6415 │ 0.8817 │ 0.6471 │\n",
      "│  k=5 │   0.6892 │      0.6892 │       0.865 │ 0.6667 │ 0.8956 │ 0.6699 │\n",
      "│  k=7 │   0.6998 │      0.6998 │      0.8685 │ 0.6789 │ 0.8998 │ 0.6804 │\n",
      "│  k=9 │   0.7042 │      0.7042 │      0.8693 │ 0.6834 │ 0.9026 │ 0.6835 │\n",
      "│ k=11 │   0.7066 │      0.7066 │      0.8698 │ 0.6867 │  0.904 │ 0.6852 │\n",
      "│ k=13 │    0.708 │       0.708 │      0.8697 │ 0.6889 │  0.905 │ 0.6862 │\n",
      "└──────┴──────────┴─────────────┴─────────────┴────────┴────────┴────────┘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 1 entry:\n",
       "  \"k\" => 13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kNN classifier\n",
    "\n",
    "# 6 different models varing k values\n",
    "parameters = [\n",
    "    Dict(\"k\" => 3),\n",
    "    Dict(\"k\" => 5),\n",
    "    Dict(\"k\" => 7),\n",
    "    Dict(\"k\" => 9),\n",
    "    Dict(\"k\" => 11),\n",
    "    Dict(\"k\" => 13),\n",
    "];\n",
    "\n",
    "common_parameters = Dict()\n",
    "\n",
    "println(\"Cross-validating kNN classifier...\")\n",
    "\n",
    "models_performance = performCrossValidationTests(parameters, common_parameters, :kNN, train_inputs, train_targets, kfoldindex)\n",
    "\n",
    "knn_best_model_parameters = getBestModelParameters(\"Fscore\", models_performance, parameters, common_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────────────────────────────────────────────────────┬──────────┬─────────────┬─────────────┬────────┬────────┬────────┐\n",
      "│\u001b[1m                                                           ANN \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m    PPV \u001b[0m│\u001b[1m    NPV \u001b[0m│\u001b[1m Fscore \u001b[0m│\n",
      "├───────────────────────────────────────────────────────────────┼──────────┼─────────────┼─────────────┼────────┼────────┼────────┤\n",
      "│ learningRate=0.01, topology=[20], transferFunctions=[NNlib.σ] │   0.7214 │      0.7214 │      0.8734 │ 0.7071 │ 0.9117 │ 0.6951 │\n",
      "└───────────────────────────────────────────────────────────────┴──────────┴─────────────┴─────────────┴────────┴────────┴────────┘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{String, Any} with 9 entries:\n",
       "  \"repetitionsTraining\" => 2\n",
       "  \"learningRate\"        => 0.01\n",
       "  \"maxEpochs\"           => 1000\n",
       "  \"topology\"            => [20]\n",
       "  \"showText\"            => false\n",
       "  \"validationRatio\"     => 0.15\n",
       "  \"maxEpochsVal\"        => 20\n",
       "  \"transferFunctions\"   => [σ]\n",
       "  \"minLoss\"             => 0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANN model\n",
    "\n",
    "# 8 different models varing topology (1-2 hidden layers), learning rate...\n",
    "parameters = [\n",
    "    Dict(\n",
    "        \"topology\"              => [8],\n",
    "        \"transferFunctions\"     => [σ],\n",
    "        \"learningRate\"          => 0.01\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [20],\n",
    "        \"transferFunctions\"     => [σ],\n",
    "        \"learningRate\"          => 0.01\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [8],\n",
    "        \"transferFunctions\"     => [σ],\n",
    "        \"learningRate\"          => 0.05\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [20],\n",
    "        \"transferFunctions\"     => [σ],\n",
    "        \"learningRate\"          => 0.05\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [7,7],\n",
    "        \"transferFunctions\"     => [σ, σ],\n",
    "        \"learningRate\"          => 0.01\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [8,4],\n",
    "        \"transferFunctions\"     => [σ, σ],\n",
    "        \"learningRate\"          => 0.01\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [7,7],\n",
    "        \"transferFunctions\"     => [σ, σ],\n",
    "        \"learningRate\"          => 0.05\n",
    "    ),\n",
    "    Dict(\n",
    "        \"topology\"              => [8,4],\n",
    "        \"transferFunctions\"     => [σ, σ],\n",
    "        \"learningRate\"          => 0.05\n",
    "    ),\n",
    "];\n",
    "\n",
    "common_parameters = Dict(\n",
    "    \"showText\" => false,\n",
    "    \"maxEpochs\"             => 1000,\n",
    "    \"minLoss\"               => 0.0,\n",
    "    \"repetitionsTraining\"   => 2,\n",
    "    \"validationRatio\"       => 0.15,\n",
    "    \"maxEpochsVal\"          => 20,\n",
    ")\n",
    "\n",
    "println(\"Cross-validating ANN classifier...\")\n",
    "\n",
    "models_performance = performCrossValidationTests(parameters, common_parameters, :ANN, train_inputs, train_targets, kfoldindex)\n",
    "\n",
    "ann_best_model_parameters = getBestModelParameters(\"Fscore\", models_performance, parameters, common_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌────────────────┬──────────┬─────────────┬─────────────┬──────────┬──────────┬──────────┐\n",
      "│\u001b[1m    Final Model \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m      PPV \u001b[0m│\u001b[1m      NPV \u001b[0m│\u001b[1m   Fscore \u001b[0m│\n",
      "├────────────────┼──────────┼─────────────┼─────────────┼──────────┼──────────┼──────────┤\n",
      "│ Final Ensemble │  0.80038 │     0.80038 │    0.902681 │ 0.798397 │ 0.929636 │ 0.796302 │\n",
      "└────────────────┴──────────┴─────────────┴─────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Performing Weighted voting classifier ensemble.\n",
    "\n",
    "println(\"Training weighed voting ensemble classifier...\")\n",
    "\n",
    "ensemble_model = trainClassEnsemble(\n",
    "    [:SVM, :DecisionTree, :kNN, :MLPC], \n",
    "    [svm_best_model_parameters, dt_best_model_parameters, knn_best_model_parameters, ann_best_model_parameters],\n",
    "    (train_inputs,train_targets),\n",
    "    [3,1,1,2]\n",
    ")\n",
    "\n",
    "# Calculate metrics with confusionMatrix() function\n",
    "(acc,_,sensitivity,specificity,PPV,NPV,f1,_) = confusionMatrix(predict(ensemble_model, test_inputs),test_targets)\n",
    "\n",
    "pretty_table(header=[\"Final Model\",\"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Fscore\"],hcat(\n",
    "    \"Final Ensemble\",\n",
    "    acc,sensitivity,specificity,PPV,NPV,f1\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: ann_best_model_parameters not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ann_best_model_parameters not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[57]:6",
      " [2] eval",
      "   @ .\\boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1428"
     ]
    }
   ],
   "source": [
    "cut_range_ordered = [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]\n",
    "\n",
    "train_targets_onehot = oneHotEncoding(train_targets, cut_range_ordered)\n",
    "test_targets_onehot = oneHotEncoding(test_targets, cut_range_ordered)\n",
    "\n",
    "println(\"Training best ANN...\")\n",
    "\n",
    "(ann,_) = trainClassANN(ann_best_model_parameters[\"topology\"],  \n",
    "    (train_inputs, train_targets_onehot),\n",
    "    testDataset = (test_inputs, test_targets_onehot), \n",
    "    transferFunctions = ann_best_model_parameters[\"transferFunctions\"], \n",
    "    maxEpochs = ann_best_model_parameters[\"maxEpochs\"], minLoss = ann_best_model_parameters[\"minLoss\"], \n",
    "    learningRate = ann_best_model_parameters[\"learningRate\"],\n",
    "    maxEpochsVal=20, showText=false)\n",
    "\n",
    "# Calculate metrics with confusionMatrix() function\n",
    "(acc,_,sensitivity,specificity,PPV,NPV,f1,_) = confusionMatrix(ann(test_inputs')',test_targets_onehot)\n",
    "\n",
    "pretty_table(header=[\"Final Model\",\"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Fscore\"],hcat(\n",
    "    \"Final ANN\",\n",
    "    acc,sensitivity,specificity,PPV,NPV,f1\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬──────────┬─────────────┬─────────────┬──────────┬──────────┬──────────┐\n",
      "│\u001b[1m Final Model \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m      PPV \u001b[0m│\u001b[1m      NPV \u001b[0m│\u001b[1m   Fscore \u001b[0m│\n",
      "├─────────────┼──────────┼─────────────┼─────────────┼──────────┼──────────┼──────────┤\n",
      "│   Final SVM │ 0.791095 │    0.791095 │    0.898641 │ 0.790276 │ 0.927041 │ 0.787215 │\n",
      "└─────────────┴──────────┴─────────────┴─────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# ScikitLearn models (SVM)\n",
    "println(\"Training best SVM...\")\n",
    "model = trainClassSklearn(:SVM, svm_best_model_parameters, train_inputs, train_targets)\n",
    "\n",
    "# Calculate metrics with confusionMatrix() function\n",
    "(acc,_,sensitivity,specificity,PPV,NPV,f1,_) = confusionMatrix(predict(model, test_inputs),test_targets)\n",
    "\n",
    "pretty_table(header=[\"Final Model\",\"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Fscore\"],hcat(\n",
    "    \"Final SVM\",\n",
    "    acc,sensitivity,specificity,PPV,NPV,f1\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COnfussionMatrix: (0.8828866849546317, 0.11711331504536822, 0.9158607350096711, 0.8573567952077873, 0.8325274725274725, 0.9293831168831169, 0.8722081510476628, [2290 381; 174 1894])\n",
      "1 class ConfusionMatrix sensitivity0.9158607350096711\n",
      "COnfussionMatrix: (0.8261236547794893, 0.17387634522051065, 0.44423963133640554, 0.9395183360700602, 0.6856330014224751, 0.8505946481665014, 0.5391498881431768, [3433 221; 603 482])\n",
      "1 class ConfusionMatrix sensitivity0.44423963133640554\n",
      "COnfussionMatrix: (0.8569318421607934, 0.1430681578392066, 0.8037542662116041, 0.8744042612839922, 0.6776978417266187, 0.9313227829202747, 0.7353629976580797, [3119 448; 230 942])\n",
      "1 class ConfusionMatrix sensitivity0.8037542662116041\n",
      "COnfussionMatrix: (0.9554758387845537, 0.0445241612154463, 0.6701570680628273, 0.9804911636447097, 0.750733137829912, 0.9713506139154161, 0.7081604426002768, [4272 85; 126 256])\n",
      "1 class ConfusionMatrix sensitivity0.6701570680628273\n",
      "COnfussionMatrix: (0.9957797003587254, 0.00422029964127453, 0.65625, 0.9980879541108987, 0.7, 0.9976640475684859, 0.6774193548387096, [4698 9; 11 21])\n",
      "1 class ConfusionMatrix sensitivity0.65625\n",
      "outerConfusionMatrix(0.7585988605190969, 0.24140113948090314, 0.7585988605190969, 0.891259649798351, 0.7531168071080021, 0.9156680659623091, 0.7475721094503749, [1894 68 102 3 1; 227 482 308 67 1; 148 71 942 10 1; 6 78 36 256 6; 0 4 2 5 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant confusion_matrix. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Int64}:\n",
       " 21    5     0    2    4\n",
       "  6  256     6   36   78\n",
       "  1    3  1894  102   68\n",
       "  1   10   148  942   71\n",
       "  1   67   227  308  482"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬──────────┬─────────────┬─────────────┬──────────┬──────────┬──────────┐\n",
      "│\u001b[1m Final Model \u001b[0m│\u001b[1m Accuracy \u001b[0m│\u001b[1m Sensitivity \u001b[0m│\u001b[1m Specificity \u001b[0m│\u001b[1m      PPV \u001b[0m│\u001b[1m      NPV \u001b[0m│\u001b[1m   Fscore \u001b[0m│\n",
      "├─────────────┼──────────┼─────────────┼─────────────┼──────────┼──────────┼──────────┤\n",
      "│    Final DT │ 0.758599 │    0.758599 │     0.89126 │ 0.753117 │ 0.915668 │ 0.747572 │\n",
      "└─────────────┴──────────┴─────────────┴─────────────┴──────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# ScikitLearn models (Decision tree)\n",
    "println(\"Training best decision tree...\")\n",
    "model = trainClassSklearn(:DecisionTree, dt_best_model_parameters, train_inputs, train_targets)\n",
    "\n",
    "# Calculate metrics with confusionMatrix() function\n",
    "(acc,_,sensitivity,specificity,PPV,NPV,f1,_) = confusionMatrix(predict(model, test_inputs),test_targets)\n",
    "\n",
    "@sk_import metrics: confusion_matrix\n",
    "\n",
    "display(confusion_matrix(test_targets, predict(model, test_inputs)))\n",
    "\n",
    "pretty_table(header=[\"Final Model\",\"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Fscore\"],hcat(\n",
    "    \"Final DT\",\n",
    "    acc,sensitivity,specificity,PPV,NPV,f1\n",
    "    )  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: knn_best_model_parameters not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: knn_best_model_parameters not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[68]:2",
      " [2] eval",
      "   @ .\\boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1428"
     ]
    }
   ],
   "source": [
    "# ScikitLearn models (KNN)\n",
    "println(\"Training best KNN...\")\n",
    "model = trainClassSklearn(:kNN, knn_best_model_parameters, train_inputs, train_targets)\n",
    "\n",
    "# Calculate metrics with confusionMatrix() function\n",
    "(acc,_,sensitivity,specificity,PPV,NPV,f1,_) = confusionMatrix(predict(model, test_inputs),test_targets)\n",
    "\n",
    "pretty_table(header=[\"Final Model\",\"Accuracy\", \"Sensitivity\", \"Specificity\", \"PPV\", \"NPV\", \"Fscore\"],hcat(\n",
    "    \"Final kNN\",\n",
    "    acc,sensitivity,specificity,PPV,NPV,f1\n",
    "    )  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06b1b7ed731f739999fce74f82a47dba7ae5cbbe55ae758f8d1f2a96cac7b7f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
